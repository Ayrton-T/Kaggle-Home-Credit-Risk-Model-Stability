{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":50160,"databundleVersionId":7921029,"sourceType":"competition"},{"sourceId":8533453,"sourceType":"datasetVersion","datasetId":5096889}],"dockerImageVersionId":30699,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":653.043013,"end_time":"2024-04-12T05:10:41.301367","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-12T04:59:48.258354","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nfrom pathlib import Path\nimport subprocess\nimport os\nimport gc\nfrom glob import glob\n\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nfrom datetime import datetime\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nROOT = '/kaggle/input/home-credit-credit-risk-model-stability'\n\nfrom sklearn.model_selection import TimeSeriesSplit, GroupKFold, StratifiedGroupKFold\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\n\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.impute import KNNImputer","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":7.704866,"end_time":"2024-04-12T04:59:59.042174","exception":false,"start_time":"2024-04-12T04:59:51.337308","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-27T20:57:19.183137Z","iopub.execute_input":"2024-05-27T20:57:19.183801Z","iopub.status.idle":"2024-05-27T20:57:23.710892Z","shell.execute_reply.started":"2024-05-27T20:57:19.183766Z","shell.execute_reply":"2024-05-27T20:57:23.710122Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score, accuracy_score\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils import data\n\nimport importlib.util\nimport sys\n\ndef load_module_from_path(module_name, file_path):\n    spec = importlib.util.spec_from_file_location(module_name, file_path)\n    module = importlib.util.module_from_spec(spec)\n    sys.modules[module_name] = module\n    spec.loader.exec_module(module)\n    return module\n\nloaded_module = load_module_from_path('model', '/kaggle/input/model/model.py')\n\nfrom model import Transformer, KAN, TransformerKAN\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2024-05-27T20:57:23.712330Z","iopub.execute_input":"2024-05-27T20:57:23.712898Z","iopub.status.idle":"2024-05-27T20:57:27.088105Z","shell.execute_reply.started":"2024-05-27T20:57:23.712872Z","shell.execute_reply":"2024-05-27T20:57:27.087103Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n\ndef load_data(data_array, batch_size, is_train=True):\n    dataset = data.TensorDataset(*data_array)\n    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n\ndef train(model, x_train, y_train, x_valid, y_valid, num_epochs, lr, batch_size, weight_decay):\n    best_model = model\n    best_validation = 0\n    train_iter = load_data((x_train, y_train), batch_size)\n    val_iter = load_data((x_valid, y_valid), batch_size, is_train=False)\n    \n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n    loss = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(5).to(device))\n    \n    for epoch in range(num_epochs):\n        model.train()\n        print(\"training\")\n        for X, y in tqdm(train_iter):\n            logits = model(X)\n            l = loss(logits, y)\n            optimizer.zero_grad()\n            l.backward()\n            optimizer.step()\n        model.eval()\n        y_prob = np.array([])\n        y_pred = np.array([])\n        cur_validation = 0\n        print(\"validation\")\n        with torch.no_grad():\n            for X, y in tqdm(val_iter):\n                m = model.predict(X)\n                pred, prob = m[0].flatten(), m[1].flatten()\n                y_prob = np.concatenate((y_prob, prob.cpu().detach().numpy()))\n                y_pred = np.concatenate((y_pred, pred.cpu().detach().numpy()))\n            cur_validation = validation(y_valid, y_prob)\n            print(f\"roc_auc_score: {cur_validation}\")\n            print(f\"accuracy: {accuracy(y_valid, y_prob)}\")\n            if cur_validation > best_validation:\n                best_validation = cur_validation\n                best_model = model\n                print(\"Save new model\")\n                torch.save(model, PATH)\n        # scheduler.step()\n    return y_prob, y_pred, best_model\n\ndef validation(y_valid, y_score):\n    return roc_auc_score(y_true=y_valid, y_score=y_score)\n\n# only use to get test batch\ndef get_batch(x, batch_size):\n    ix = torch.arange(0, len(x), batch_size)\n    return [x[i:i+batch_size] for i in ix]\n\ndef accuracy(y_true, y_pred):\n    y_pred = np.where(y_pred > 0.5, 1., 0.)\n    return accuracy_score(y_true=y_true, y_pred=y_pred)\n\ndef predict(model, x_test, batch_size):\n    test_iter = get_batch(x_test, batch_size)\n    res = np.array([])\n    model.eval()\n    with torch.no_grad():\n        for X in test_iter:\n            X = X.to(device)  # 確保數據位於相同設備\n            label, prob = model.predict(X)\n            res = np.concatenate((res, prob.cpu().flatten().numpy()))  # 將結果移回 CPU 並存入 res\n            \n    ret = res[:len(x_test)] # 確保輸出與輸入長度相同\n#     print(ret.shape)\n    return ret","metadata":{"execution":{"iopub.status.busy":"2024-05-27T20:57:27.089302Z","iopub.execute_input":"2024-05-27T20:57:27.089849Z","iopub.status.idle":"2024-05-27T20:57:27.106673Z","shell.execute_reply.started":"2024-05-27T20:57:27.089821Z","shell.execute_reply":"2024-05-27T20:57:27.105738Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class Pipeline:\n    def set_table_dtypes(df):\n        for col in df.columns:\n            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Int64))\n            elif col in [\"date_decision\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Date))\n            elif col[-1] in (\"P\", \"A\"):\n                df = df.with_columns(pl.col(col).cast(pl.Float64))\n            elif col[-1] in (\"M\",):\n                df = df.with_columns(pl.col(col).cast(pl.String))\n            elif col[-1] in (\"D\",):\n                df = df.with_columns(pl.col(col).cast(pl.Date))\n        return df\n\n    def handle_dates(df):\n        for col in df.columns:\n            if col[-1] in (\"D\",):\n                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))  #!!?\n                df = df.with_columns(pl.col(col).dt.total_days()) # t - t-1\n        df = df.drop(\"date_decision\", \"MONTH\")\n        return df\n\n    def filter_cols(df):\n        for col in df.columns:\n            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n                isnull = df[col].is_null().mean()\n                if isnull > 0.7:\n                    df = df.drop(col)\n        \n        for col in df.columns:\n            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n                freq = df[col].n_unique()\n                if (freq == 1) | (freq > 200):\n                    df = df.drop(col)\n        \n        return df\n\n\nclass Aggregator:\n    #Please add or subtract features yourself, be aware that too many features will take up too much space.\n    def num_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n        \n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n        return expr_max +expr_last+expr_mean\n    \n    def date_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"D\")]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n        return  expr_max +expr_last+expr_mean\n    \n    def str_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"M\",)]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        #expr_count = [pl.count(col).alias(f\"count_{col}\") for col in cols]\n        return  expr_max +expr_last#+expr_count\n    \n    def other_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        return  expr_max +expr_last\n    \n    def count_expr(df):\n        cols = [col for col in df.columns if \"num_group\" in col]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols] \n        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        return  expr_max +expr_last\n    \n    def get_exprs(df):\n        exprs = Aggregator.num_expr(df) + \\\n                Aggregator.date_expr(df) + \\\n                Aggregator.str_expr(df) + \\\n                Aggregator.other_expr(df) + \\\n                Aggregator.count_expr(df)\n\n        return exprs\n\ndef read_file(path, depth=None):\n    df = pl.read_parquet(path)\n    df = df.pipe(Pipeline.set_table_dtypes)\n    if depth in [1,2]:\n        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df)) \n    return df\n\ndef read_files(regex_path, depth=None):\n    chunks = []\n    \n    for path in glob(str(regex_path)):\n        df = pl.read_parquet(path)\n        df = df.pipe(Pipeline.set_table_dtypes)\n        if depth in [1, 2]:\n            df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n        chunks.append(df)\n    \n    df = pl.concat(chunks, how=\"vertical_relaxed\")\n    df = df.unique(subset=[\"case_id\"])\n    return df\n\ndef feature_eng(df_base, depth_0, depth_1, depth_2):\n    df_base = (\n        df_base\n        .with_columns(\n            month_decision = pl.col(\"date_decision\").dt.month(),\n            weekday_decision = pl.col(\"date_decision\").dt.weekday(),\n        )\n    )\n    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n    df_base = df_base.pipe(Pipeline.handle_dates)\n    return df_base\n\ndef to_pandas(df_data, cat_cols=None):\n    df_data = df_data.to_pandas()\n    if cat_cols is None:\n        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n    return df_data, cat_cols\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        if str(col_type)==\"category\":\n            continue\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            continue\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","metadata":{"papermill":{"duration":0.04765,"end_time":"2024-04-12T04:59:59.095877","exception":false,"start_time":"2024-04-12T04:59:59.048227","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-27T20:57:27.108887Z","iopub.execute_input":"2024-05-27T20:57:27.109151Z","iopub.status.idle":"2024-05-27T20:57:27.147970Z","shell.execute_reply.started":"2024-05-27T20:57:27.109128Z","shell.execute_reply":"2024-05-27T20:57:27.147176Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"ROOT            = Path(\"/kaggle/input/home-credit-credit-risk-model-stability\")\n\nTRAIN_DIR       = ROOT / \"parquet_files\" / \"train\"\nTEST_DIR        = ROOT / \"parquet_files\" / \"test\"","metadata":{"papermill":{"duration":0.011907,"end_time":"2024-04-12T04:59:59.1133","exception":false,"start_time":"2024-04-12T04:59:59.101393","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-27T20:57:27.149054Z","iopub.execute_input":"2024-05-27T20:57:27.149377Z","iopub.status.idle":"2024-05-27T20:57:27.159300Z","shell.execute_reply.started":"2024-05-27T20:57:27.149346Z","shell.execute_reply":"2024-05-27T20:57:27.158472Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"%%time\ndata_store = {\n    \"df_base\": read_file(TRAIN_DIR / \"train_base.parquet\"),\n    \"depth_0\": [\n        read_file(TRAIN_DIR / \"train_static_cb_0.parquet\"),\n        read_files(TRAIN_DIR / \"train_static_0_*.parquet\"),\n    ],\n    \"depth_1\": [\n        read_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),\n        read_files(TRAIN_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_other_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_person_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_deposit_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),\n    ],\n    \"depth_2\": [\n        read_file(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n        read_files(TRAIN_DIR / \"train_credit_bureau_a_2_*.parquet\", 2),\n        read_file(TRAIN_DIR / \"train_applprev_2.parquet\", 2),\n        read_file(TRAIN_DIR / \"train_person_2.parquet\", 2)\n    ]\n}","metadata":{"papermill":{"duration":138.440655,"end_time":"2024-04-12T05:02:17.559186","exception":false,"start_time":"2024-04-12T04:59:59.118531","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-27T20:57:27.160239Z","iopub.execute_input":"2024-05-27T20:57:27.160519Z","iopub.status.idle":"2024-05-27T20:59:45.048756Z","shell.execute_reply.started":"2024-05-27T20:57:27.160497Z","shell.execute_reply":"2024-05-27T20:59:45.047738Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"CPU times: user 4min 49s, sys: 1min 37s, total: 6min 27s\nWall time: 2min 17s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\ndf_train = feature_eng(**data_store)\nprint(\"train data shape:\\t\", df_train.shape)\ndel data_store\ngc.collect()\ndf_train = df_train.pipe(Pipeline.filter_cols)\ndf_train, cat_cols = to_pandas(df_train)\ndf_train = reduce_mem_usage(df_train)\nprint(\"train data shape:\\t\", df_train.shape)\nnums=df_train.select_dtypes(exclude='category').columns\nfrom itertools import combinations, permutations\n#df_train=df_train[nums]\nnans_df = df_train[nums].isna()\nnans_groups={}\nfor col in nums:\n    cur_group = nans_df[col].sum()\n    try:\n        nans_groups[cur_group].append(col)\n    except:\n        nans_groups[cur_group]=[col]\ndel nans_df; x=gc.collect()\n\ndef reduce_group(grps):\n    use = []\n    for g in grps:\n        mx = 0; vx = g[0]\n        for gg in g:\n            n = df_train[gg].nunique()\n            if n>mx:\n                mx = n\n                vx = gg\n            #print(str(gg)+'-'+str(n),', ',end='')\n        use.append(vx)\n        #print()\n    print('Use these',use)\n    return use\n\ndef group_columns_by_correlation(matrix, threshold=0.8):\n    # 计算列之间的相关性\n    correlation_matrix = matrix.corr()\n\n    # 分组列\n    groups = []\n    remaining_cols = list(matrix.columns)\n    while remaining_cols:\n        col = remaining_cols.pop(0)\n        group = [col]\n        correlated_cols = [col]\n        for c in remaining_cols:\n            if correlation_matrix.loc[col, c] >= threshold:\n                group.append(c)\n                correlated_cols.append(c)\n        groups.append(group)\n        remaining_cols = [c for c in remaining_cols if c not in correlated_cols]\n    \n    return groups\n\nuses=[]\nfor k,v in nans_groups.items():\n    if len(v)>1:\n            Vs = nans_groups[k]\n            #cross_features=list(combinations(Vs, 2))\n            #make_corr(Vs)\n            grps= group_columns_by_correlation(df_train[Vs], threshold=0.8)\n            use=reduce_group(grps)\n            uses=uses+use\n            #make_corr(use)\n    else:\n        uses=uses+v\n    print('####### NAN count =',k)\nprint(uses)\nprint(len(uses))\nuses=uses+list(df_train.select_dtypes(include='category').columns)\nprint(len(uses))\ndf_train=df_train[uses]","metadata":{"papermill":{"duration":101.102321,"end_time":"2024-04-12T05:03:58.666792","exception":false,"start_time":"2024-04-12T05:02:17.564471","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-27T20:59:45.050189Z","iopub.execute_input":"2024-05-27T20:59:45.050644Z","iopub.status.idle":"2024-05-27T21:01:24.861577Z","shell.execute_reply.started":"2024-05-27T20:59:45.050611Z","shell.execute_reply":"2024-05-27T21:01:24.860688Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"train data shape:\t (1526659, 861)\nMemory usage of dataframe is 4322.75 MB\nMemory usage after optimization is: 1528.81 MB\nDecreased by 64.6%\ntrain data shape:\t (1526659, 472)\nUse these ['case_id', 'WEEK_NUM', 'target', 'month_decision', 'weekday_decision', 'credamount_770A', 'applicationcnt_361L', 'applications30d_658L', 'applicationscnt_1086L', 'applicationscnt_464L', 'applicationscnt_867L', 'clientscnt_1022L', 'clientscnt_100L', 'clientscnt_1071L', 'clientscnt_1130L', 'clientscnt_157L', 'clientscnt_257L', 'clientscnt_304L', 'clientscnt_360L', 'clientscnt_493L', 'clientscnt_533L', 'clientscnt_887L', 'clientscnt_946L', 'deferredmnthsnum_166L', 'disbursedcredamount_1113A', 'downpmt_116A', 'homephncnt_628L', 'isbidproduct_1095L', 'mobilephncnt_593L', 'numactivecreds_622L', 'numactivecredschannel_414L', 'numactiverelcontr_750L', 'numcontrs3months_479L', 'numnotactivated_1143L', 'numpmtchanneldd_318L', 'numrejects9m_859L', 'sellerplacecnt_915L', 'max_mainoccupationinc_384A', 'max_birth_259D', 'max_num_group1_9']\n####### NAN count = 0\n####### NAN count = 918788\nUse these ['dateofbirth_337D', 'days180_256L', 'days30_165L', 'days360_512L', 'firstquarter_103L', 'fourthquarter_440L', 'secondquarter_766L', 'thirdquarter_1082L', 'max_debtoutstand_525A', 'max_debtoverdue_47A', 'max_refreshdate_3813885D', 'mean_refreshdate_3813885D']\n####### NAN count = 140968\nUse these ['pmtscount_423L', 'pmtssum_45A']\n####### NAN count = 954021\n####### NAN count = 806659\n####### NAN count = 866332\n####### NAN count = 418178\nUse these ['amtinstpaidbefduel24m_4187115A', 'numinstlswithdpd5_4187116L']\n####### NAN count = 561124\nUse these ['annuitynextmonth_57A', 'currdebt_22A', 'currdebtcredtyperange_828A', 'numinstls_657L', 'totalsettled_863A']\n####### NAN count = 4\nUse these ['mindbddpdlast24m_3658935P']\n####### NAN count = 613202\n####### NAN count = 948244\nUse these ['mindbdtollast24m_4525191P']\n####### NAN count = 972827\n####### NAN count = 467175\nUse these ['avginstallast24m_3658937A', 'maxinstallast24m_3658928A']\n####### NAN count = 624875\n####### NAN count = 757006\n####### NAN count = 841181\n####### NAN count = 1026987\n####### NAN count = 455190\n####### NAN count = 460822\nUse these ['commnoinclast6m_3546845L', 'maxdpdfrom6mto36m_3546853P']\n####### NAN count = 343375\n####### NAN count = 833735\n####### NAN count = 887659\nUse these ['daysoverduetolerancedd_3976961L', 'numinsttopaygr_769L']\n####### NAN count = 452594\n####### NAN count = 977119\nUse these ['eir_270L']\n####### NAN count = 190833\n####### NAN count = 859214\n####### NAN count = 482103\n####### NAN count = 453587\nUse these ['lastapplicationdate_877D', 'mean_creationdate_885D', 'max_num_group1', 'last_num_group1', 'max_num_group2_14', 'last_num_group2_14']\n####### NAN count = 305137\nUse these ['lastapprcredamount_781A', 'lastapprdate_640D']\n####### NAN count = 442041\n####### NAN count = 977975\nUse these ['lastrejectcredamount_222A', 'lastrejectdate_50D']\n####### NAN count = 769046\n####### NAN count = 511255\nUse these ['mastercontrelectronic_519L', 'mastercontrexist_109L', 'maxannuity_159A', 'maxdebt4_972A', 'maxdpdlast24m_143P', 'maxdpdlast3m_392P', 'maxdpdtolerance_374P']\n####### NAN count = 306019\n####### NAN count = 960953\n####### NAN count = 705504\n####### NAN count = 876276\n####### NAN count = 826000\n####### NAN count = 829402\n####### NAN count = 1032856\n####### NAN count = 766958\nUse these ['numinstpaidearly_338L', 'numinstpaidearly5d_1087L', 'numinstpaidlate1d_3546852L']\n####### NAN count = 452593\n####### NAN count = 455081\nUse these ['numinstlsallpaid_934L']\n####### NAN count = 445669\nUse these ['numinstlswithdpd10_728L', 'numinstlswithoutdpd_562L']\n####### NAN count = 456495\nUse these ['numinstpaid_4499208L']\n####### NAN count = 847191\n####### NAN count = 446983\nUse these ['numinstregularpaidest_4493210L', 'numinstpaidearly5dest_4493211L', 'sumoutstandtotalest_4493215A']\n####### NAN count = 840646\n####### NAN count = 669186\n####### NAN count = 455612\nUse these ['pctinstlsallpaidearl3d_427L', 'pctinstlsallpaidlate1d_3546856L']\n####### NAN count = 458738\n####### NAN count = 461362\n####### NAN count = 459827\n####### NAN count = 460079\n####### NAN count = 44954\n####### NAN count = 78526\n####### NAN count = 131888\n####### NAN count = 181122\n####### NAN count = 223240\n####### NAN count = 445320\n####### NAN count = 3\nUse these ['mean_actualdpd_943P']\n####### NAN count = 305154\nUse these ['max_annuity_853A', 'mean_annuity_853A']\n####### NAN count = 308739\nUse these ['max_credacc_credlmt_575A', 'max_credamount_590A', 'max_downpmt_134A', 'mean_credacc_credlmt_575A', 'mean_credamount_590A', 'mean_downpmt_134A']\n####### NAN count = 307441\nUse these ['max_currdebt_94A', 'mean_currdebt_94A']\n####### NAN count = 419006\nUse these ['max_mainoccupationinc_437A', 'mean_mainoccupationinc_437A']\n####### NAN count = 306361\nUse these ['mean_maxdpdtolerance_577P']\n####### NAN count = 450969\nUse these ['max_outstandingdebt_522A', 'mean_outstandingdebt_522A']\n####### NAN count = 420383\n####### NAN count = 307551\n####### NAN count = 477657\n####### NAN count = 433335\nUse these ['last_credamount_590A', 'last_downpmt_134A']\n####### NAN count = 438219\n####### NAN count = 824731\n####### NAN count = 312491\n####### NAN count = 899665\n####### NAN count = 827764\nUse these ['max_approvaldate_319D', 'mean_approvaldate_319D']\n####### NAN count = 442999\nUse these ['max_dateactivated_425D', 'mean_dateactivated_425D']\n####### NAN count = 454678\nUse these ['max_dtlastpmt_581D', 'mean_dtlastpmt_581D']\n####### NAN count = 703840\nUse these ['max_dtlastpmtallstes_3545839D', 'mean_dtlastpmtallstes_3545839D']\n####### NAN count = 548987\nUse these ['max_employedfrom_700D']\n####### NAN count = 559169\nUse these ['max_firstnonzeroinstldate_307D', 'mean_firstnonzeroinstldate_307D']\n####### NAN count = 334873\n####### NAN count = 891021\n####### NAN count = 305203\n####### NAN count = 920818\n####### NAN count = 1016761\n####### NAN count = 1050001\n####### NAN count = 485683\n####### NAN count = 961606\n####### NAN count = 552766\nUse these ['max_pmtnum_8L']\n####### NAN count = 321446\nUse these ['last_pmtnum_8L']\n####### NAN count = 482174\nUse these ['max_pmtamount_36A', 'last_pmtamount_36A', 'max_processingdate_168D', 'last_processingdate_168D', 'max_num_group1_5']\n####### NAN count = 1044394\nUse these ['mean_credlmt_230A']\n####### NAN count = 1036944\nUse these ['mean_credlmt_935A']\n####### NAN count = 603001\nUse these ['mean_pmts_dpd_1073P', 'max_dpdmaxdatemonth_89T', 'max_dpdmaxdateyear_596T']\n####### NAN count = 263166\nUse these ['max_pmts_dpd_303P', 'mean_dpdmax_757P', 'max_dpdmaxdatemonth_442T', 'max_dpdmaxdateyear_896T', 'mean_pmts_dpd_303P']\n####### NAN count = 514070\nUse these ['mean_instlamount_768A']\n####### NAN count = 606920\nUse these ['mean_monthlyinstlamount_332A']\n####### NAN count = 263233\nUse these ['max_monthlyinstlamount_674A', 'mean_monthlyinstlamount_674A']\n####### NAN count = 517511\nUse these ['mean_outstandingamount_354A']\n####### NAN count = 545885\nUse these ['mean_outstandingamount_362A']\n####### NAN count = 636453\nUse these ['mean_overdueamount_31A']\n####### NAN count = 512650\nUse these ['mean_overdueamount_659A', 'max_numberofoverdueinstls_725L']\n####### NAN count = 263171\nUse these ['mean_overdueamountmax2_14A', 'mean_totaloutstanddebtvalue_39A', 'mean_dateofcredend_289D', 'mean_dateofcredstart_739D', 'max_lastupdate_1112D', 'mean_lastupdate_1112D', 'max_numberofcontrsvalue_258L', 'max_numberofoverdueinstlmax_1039L', 'max_overdueamountmaxdatemonth_365T', 'max_overdueamountmaxdateyear_2T', 'mean_pmts_overdue_1140A', 'max_pmts_month_158T', 'max_pmts_year_1139T']\n####### NAN count = 262653\nUse these ['mean_overdueamountmax2_398A', 'max_dateofcredend_353D', 'max_dateofcredstart_181D', 'mean_dateofcredend_353D', 'max_numberofoverdueinstlmax_1151L']\n####### NAN count = 512590\nUse these ['mean_overdueamountmax_35A', 'max_overdueamountmaxdatemonth_284T', 'max_overdueamountmaxdateyear_994T', 'mean_pmts_overdue_1152A']\n####### NAN count = 513987\nUse these ['max_residualamount_488A']\n####### NAN count = 1039597\nUse these ['mean_residualamount_856A']\n####### NAN count = 606900\nUse these ['max_totalamount_6A', 'mean_totalamount_6A']\n####### NAN count = 545855\nUse these ['mean_totalamount_996A']\n####### NAN count = 636448\nUse these ['mean_totaldebtoverduevalue_718A', 'mean_totaloutstanddebtvalue_668A', 'max_numberofcontrsvalue_358L']\n####### NAN count = 297072\nUse these ['max_dateofrealrepmt_138D', 'mean_dateofrealrepmt_138D']\n####### NAN count = 512961\nUse these ['max_lastupdate_388D', 'mean_lastupdate_388D']\n####### NAN count = 512591\nUse these ['max_numberofoverdueinstlmaxdat_148D']\n####### NAN count = 802351\nUse these ['mean_numberofoverdueinstlmaxdat_641D']\n####### NAN count = 1012361\nUse these ['mean_overdueamountmax2date_1002D']\n####### NAN count = 806653\nUse these ['max_overdueamountmax2date_1142D']\n####### NAN count = 1007594\n####### NAN count = 553734\n####### NAN count = 822517\n####### NAN count = 745109\n####### NAN count = 545898\n####### NAN count = 636545\n####### NAN count = 545895\n####### NAN count = 636544\n####### NAN count = 512657\n####### NAN count = 561307\n####### NAN count = 649082\nUse these ['last_num_group1_6']\n####### NAN count = 140386\nUse these ['last_mainoccupationinc_384A', 'last_birth_259D']\n####### NAN count = 750301\nUse these ['max_empl_employedfrom_271D']\n####### NAN count = 959958\nUse these ['last_personindex_1023L']\n####### NAN count = 587206\n####### NAN count = 772\n####### NAN count = 262659\n####### NAN count = 512884\nUse these ['max_pmts_month_706T', 'max_pmts_year_507T']\n####### NAN count = 512598\nUse these ['last_pmts_month_158T', 'last_pmts_year_1139T']\n####### NAN count = 994041\nUse these ['last_pmts_month_706T', 'last_pmts_year_507T']\n####### NAN count = 634357\nUse these ['max_num_group1_13', 'max_num_group2_13', 'last_num_group2_13']\n####### NAN count = 141371\nUse these ['max_num_group1_15', 'max_num_group2_15']\n####### NAN count = 91554\n['case_id', 'WEEK_NUM', 'target', 'month_decision', 'weekday_decision', 'credamount_770A', 'applicationcnt_361L', 'applications30d_658L', 'applicationscnt_1086L', 'applicationscnt_464L', 'applicationscnt_867L', 'clientscnt_1022L', 'clientscnt_100L', 'clientscnt_1071L', 'clientscnt_1130L', 'clientscnt_157L', 'clientscnt_257L', 'clientscnt_304L', 'clientscnt_360L', 'clientscnt_493L', 'clientscnt_533L', 'clientscnt_887L', 'clientscnt_946L', 'deferredmnthsnum_166L', 'disbursedcredamount_1113A', 'downpmt_116A', 'homephncnt_628L', 'isbidproduct_1095L', 'mobilephncnt_593L', 'numactivecreds_622L', 'numactivecredschannel_414L', 'numactiverelcontr_750L', 'numcontrs3months_479L', 'numnotactivated_1143L', 'numpmtchanneldd_318L', 'numrejects9m_859L', 'sellerplacecnt_915L', 'max_mainoccupationinc_384A', 'max_birth_259D', 'max_num_group1_9', 'birthdate_574D', 'dateofbirth_337D', 'days180_256L', 'days30_165L', 'days360_512L', 'firstquarter_103L', 'fourthquarter_440L', 'secondquarter_766L', 'thirdquarter_1082L', 'max_debtoutstand_525A', 'max_debtoverdue_47A', 'max_refreshdate_3813885D', 'mean_refreshdate_3813885D', 'pmtscount_423L', 'pmtssum_45A', 'responsedate_1012D', 'responsedate_4527233D', 'actualdpdtolerance_344P', 'amtinstpaidbefduel24m_4187115A', 'numinstlswithdpd5_4187116L', 'annuitynextmonth_57A', 'currdebt_22A', 'currdebtcredtyperange_828A', 'numinstls_657L', 'totalsettled_863A', 'mindbddpdlast24m_3658935P', 'avgdbddpdlast3m_4187120P', 'mindbdtollast24m_4525191P', 'avgdpdtolclosure24_3658938P', 'avginstallast24m_3658937A', 'maxinstallast24m_3658928A', 'avgmaxdpdlast9m_3716943P', 'avgoutstandbalancel6m_4187114A', 'avgpmtlast12m_4525200A', 'cntincpaycont9m_3716944L', 'cntpmts24_3658933L', 'commnoinclast6m_3546845L', 'maxdpdfrom6mto36m_3546853P', 'datefirstoffer_1144D', 'datelastunpaid_3546854D', 'daysoverduetolerancedd_3976961L', 'numinsttopaygr_769L', 'dtlastpmtallstes_4499206D', 'eir_270L', 'firstclxcampaign_1125D', 'firstdatedue_489D', 'lastactivateddate_801D', 'lastapplicationdate_877D', 'mean_creationdate_885D', 'max_num_group1', 'last_num_group1', 'max_num_group2_14', 'last_num_group2_14', 'lastapprcredamount_781A', 'lastapprdate_640D', 'lastdelinqdate_224D', 'lastrejectcredamount_222A', 'lastrejectdate_50D', 'maininc_215A', 'mastercontrelectronic_519L', 'mastercontrexist_109L', 'maxannuity_159A', 'maxdebt4_972A', 'maxdpdlast24m_143P', 'maxdpdlast3m_392P', 'maxdpdtolerance_374P', 'maxdbddpdlast1m_3658939P', 'maxdbddpdtollast12m_3658940P', 'maxdbddpdtollast6m_4187119P', 'maxdpdinstldate_3546855D', 'maxdpdinstlnum_3546846P', 'maxlnamtstart6m_4525199A', 'maxoutstandbalancel12m_4187113A', 'numinstpaidearly_338L', 'numinstpaidearly5d_1087L', 'numinstpaidlate1d_3546852L', 'numincomingpmts_3546848L', 'numinstlsallpaid_934L', 'numinstlswithdpd10_728L', 'numinstlswithoutdpd_562L', 'numinstpaid_4499208L', 'numinstpaidearly3d_3546850L', 'numinstregularpaidest_4493210L', 'numinstpaidearly5dest_4493211L', 'sumoutstandtotalest_4493215A', 'numinstpaidlastcontr_4325080L', 'numinstregularpaid_973L', 'pctinstlsallpaidearl3d_427L', 'pctinstlsallpaidlate1d_3546856L', 'pctinstlsallpaidlat10d_839L', 'pctinstlsallpaidlate4d_3546849L', 'pctinstlsallpaidlate6d_3546844L', 'pmtnum_254L', 'posfpd10lastmonth_333P', 'posfpd30lastmonth_3976960P', 'posfstqpd30lastmonth_3976962P', 'price_1097A', 'sumoutstandtotal_3546847A', 'totaldebt_9A', 'mean_actualdpd_943P', 'max_annuity_853A', 'mean_annuity_853A', 'max_credacc_credlmt_575A', 'max_credamount_590A', 'max_downpmt_134A', 'mean_credacc_credlmt_575A', 'mean_credamount_590A', 'mean_downpmt_134A', 'max_currdebt_94A', 'mean_currdebt_94A', 'max_mainoccupationinc_437A', 'mean_mainoccupationinc_437A', 'mean_maxdpdtolerance_577P', 'max_outstandingdebt_522A', 'mean_outstandingdebt_522A', 'last_actualdpd_943P', 'last_annuity_853A', 'last_credacc_credlmt_575A', 'last_credamount_590A', 'last_downpmt_134A', 'last_currdebt_94A', 'last_mainoccupationinc_437A', 'last_maxdpdtolerance_577P', 'last_outstandingdebt_522A', 'max_approvaldate_319D', 'mean_approvaldate_319D', 'max_dateactivated_425D', 'mean_dateactivated_425D', 'max_dtlastpmt_581D', 'mean_dtlastpmt_581D', 'max_dtlastpmtallstes_3545839D', 'mean_dtlastpmtallstes_3545839D', 'max_employedfrom_700D', 'max_firstnonzeroinstldate_307D', 'mean_firstnonzeroinstldate_307D', 'last_approvaldate_319D', 'last_creationdate_885D', 'last_dateactivated_425D', 'last_dtlastpmtallstes_3545839D', 'last_employedfrom_700D', 'last_firstnonzeroinstldate_307D', 'max_byoccupationinc_3656910L', 'max_childnum_21L', 'max_pmtnum_8L', 'last_pmtnum_8L', 'max_pmtamount_36A', 'last_pmtamount_36A', 'max_processingdate_168D', 'last_processingdate_168D', 'max_num_group1_5', 'mean_credlmt_230A', 'mean_credlmt_935A', 'mean_pmts_dpd_1073P', 'max_dpdmaxdatemonth_89T', 'max_dpdmaxdateyear_596T', 'max_pmts_dpd_303P', 'mean_dpdmax_757P', 'max_dpdmaxdatemonth_442T', 'max_dpdmaxdateyear_896T', 'mean_pmts_dpd_303P', 'mean_instlamount_768A', 'mean_monthlyinstlamount_332A', 'max_monthlyinstlamount_674A', 'mean_monthlyinstlamount_674A', 'mean_outstandingamount_354A', 'mean_outstandingamount_362A', 'mean_overdueamount_31A', 'mean_overdueamount_659A', 'max_numberofoverdueinstls_725L', 'mean_overdueamountmax2_14A', 'mean_totaloutstanddebtvalue_39A', 'mean_dateofcredend_289D', 'mean_dateofcredstart_739D', 'max_lastupdate_1112D', 'mean_lastupdate_1112D', 'max_numberofcontrsvalue_258L', 'max_numberofoverdueinstlmax_1039L', 'max_overdueamountmaxdatemonth_365T', 'max_overdueamountmaxdateyear_2T', 'mean_pmts_overdue_1140A', 'max_pmts_month_158T', 'max_pmts_year_1139T', 'mean_overdueamountmax2_398A', 'max_dateofcredend_353D', 'max_dateofcredstart_181D', 'mean_dateofcredend_353D', 'max_numberofoverdueinstlmax_1151L', 'mean_overdueamountmax_35A', 'max_overdueamountmaxdatemonth_284T', 'max_overdueamountmaxdateyear_994T', 'mean_pmts_overdue_1152A', 'max_residualamount_488A', 'mean_residualamount_856A', 'max_totalamount_6A', 'mean_totalamount_6A', 'mean_totalamount_996A', 'mean_totaldebtoverduevalue_718A', 'mean_totaloutstanddebtvalue_668A', 'max_numberofcontrsvalue_358L', 'max_dateofrealrepmt_138D', 'mean_dateofrealrepmt_138D', 'max_lastupdate_388D', 'mean_lastupdate_388D', 'max_numberofoverdueinstlmaxdat_148D', 'mean_numberofoverdueinstlmaxdat_641D', 'mean_overdueamountmax2date_1002D', 'max_overdueamountmax2date_1142D', 'last_refreshdate_3813885D', 'max_nominalrate_281L', 'max_nominalrate_498L', 'max_numberofinstls_229L', 'max_numberofinstls_320L', 'max_numberofoutstandinstls_520L', 'max_numberofoutstandinstls_59L', 'max_numberofoverdueinstls_834L', 'max_periodicityofpmts_1102L', 'max_periodicityofpmts_837L', 'last_num_group1_6', 'last_mainoccupationinc_384A', 'last_birth_259D', 'max_empl_employedfrom_271D', 'last_personindex_1023L', 'last_persontype_1072L', 'max_collater_valueofguarantee_1124L', 'max_collater_valueofguarantee_876L', 'max_pmts_month_706T', 'max_pmts_year_507T', 'last_pmts_month_158T', 'last_pmts_year_1139T', 'last_pmts_month_706T', 'last_pmts_year_507T', 'max_num_group1_13', 'max_num_group2_13', 'last_num_group2_13', 'max_num_group1_15', 'max_num_group2_15']\n276\n389\nCPU times: user 1min 49s, sys: 23.8 s, total: 2min 12s\nWall time: 1min 39s\n","output_type":"stream"}]},{"cell_type":"code","source":"sample = pd.read_csv(\"/kaggle/input/home-credit-credit-risk-model-stability/sample_submission.csv\")\ndevice='gpu'\n#n_samples=200000\nn_est=6000\nDRY_RUN = True if sample.shape[0] == 10 else False   \nif DRY_RUN:\n    device='cpu'\n    df_train = df_train.iloc[:50000]\n    #n_samples=10000\n    n_est=600\nprint(device)","metadata":{"papermill":{"duration":0.02927,"end_time":"2024-04-12T05:03:58.705129","exception":false,"start_time":"2024-04-12T05:03:58.675859","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-27T21:01:24.862796Z","iopub.execute_input":"2024-05-27T21:01:24.863083Z","iopub.status.idle":"2024-05-27T21:01:24.878883Z","shell.execute_reply.started":"2024-05-27T21:01:24.863059Z","shell.execute_reply":"2024-05-27T21:01:24.878018Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"cpu\n","output_type":"stream"}]},{"cell_type":"code","source":"data_store = {\n    \"df_base\": read_file(TEST_DIR / \"test_base.parquet\"),\n    \"depth_0\": [\n        read_file(TEST_DIR / \"test_static_cb_0.parquet\"),\n        read_files(TEST_DIR / \"test_static_0_*.parquet\"),\n    ],\n    \"depth_1\": [\n        read_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1),\n        read_file(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),\n        read_files(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1),\n        read_file(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_other_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_person_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_deposit_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_debitcard_1.parquet\", 1),\n    ],\n    \"depth_2\": [\n        read_file(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),\n        read_files(TEST_DIR / \"test_credit_bureau_a_2_*.parquet\", 2),\n        read_file(TEST_DIR / \"test_applprev_2.parquet\", 2),\n        read_file(TEST_DIR / \"test_person_2.parquet\", 2)\n    ]\n}","metadata":{"papermill":{"duration":0.416482,"end_time":"2024-04-12T05:03:59.130015","exception":false,"start_time":"2024-04-12T05:03:58.713533","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-27T21:01:24.880003Z","iopub.execute_input":"2024-05-27T21:01:24.880257Z","iopub.status.idle":"2024-05-27T21:01:25.196055Z","shell.execute_reply.started":"2024-05-27T21:01:24.880234Z","shell.execute_reply":"2024-05-27T21:01:25.195129Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"df_test = feature_eng(**data_store)\nprint(\"test data shape:\\t\", df_test.shape)\ndel data_store\ngc.collect()\ndf_test = df_test.select([col for col in df_train.columns if col != \"target\"])\nprint(\"train data shape:\\t\", df_train.shape)\nprint(\"test data shape:\\t\", df_test.shape)\n\ndf_test, cat_cols = to_pandas(df_test, cat_cols)\ndf_test = reduce_mem_usage(df_test)\n\ngc.collect()","metadata":{"papermill":{"duration":0.524809,"end_time":"2024-04-12T05:03:59.665322","exception":false,"start_time":"2024-04-12T05:03:59.140513","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-27T21:01:25.199405Z","iopub.execute_input":"2024-05-27T21:01:25.199721Z","iopub.status.idle":"2024-05-27T21:01:25.823319Z","shell.execute_reply.started":"2024-05-27T21:01:25.199697Z","shell.execute_reply":"2024-05-27T21:01:25.822379Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"test data shape:\t (10, 860)\ntrain data shape:\t (50000, 389)\ntest data shape:\t (10, 388)\nMemory usage of dataframe is 0.04 MB\nMemory usage after optimization is: 0.02 MB\nDecreased by 40.3%\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"markdown","source":"### Feature Selection","metadata":{"papermill":{"duration":0.008967,"end_time":"2024-04-12T05:03:59.683371","exception":false,"start_time":"2024-04-12T05:03:59.674404","status":"completed"},"tags":[]}},{"cell_type":"code","source":"y = df_train[\"target\"]\nweeks = df_train[\"WEEK_NUM\"]\ndf_train= df_train.drop(columns=[\"target\", \"case_id\", \"WEEK_NUM\"])\ncv = StratifiedGroupKFold(n_splits=5, shuffle=False)\n\ntest_id = df_test[\"case_id\"]\ntest_week = df_test[\"WEEK_NUM\"]\n\ndf_test = df_test.drop(columns=[\"case_id\", \"WEEK_NUM\"])\n","metadata":{"papermill":{"duration":0.139261,"end_time":"2024-04-12T05:03:59.831504","exception":false,"start_time":"2024-04-12T05:03:59.692243","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-27T21:01:25.824578Z","iopub.execute_input":"2024-05-27T21:01:25.824930Z","iopub.status.idle":"2024-05-27T21:01:25.977396Z","shell.execute_reply.started":"2024-05-27T21:01:25.824896Z","shell.execute_reply":"2024-05-27T21:01:25.976501Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df_train[cat_cols] = df_train[cat_cols].astype(str)\ndf_test[cat_cols] = df_test[cat_cols].astype(str)","metadata":{"papermill":{"duration":0.306032,"end_time":"2024-04-12T05:04:00.146706","exception":false,"start_time":"2024-04-12T05:03:59.840674","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-27T21:01:25.978720Z","iopub.execute_input":"2024-05-27T21:01:25.979041Z","iopub.status.idle":"2024-05-27T21:01:26.276167Z","shell.execute_reply.started":"2024-05-27T21:01:25.979012Z","shell.execute_reply":"2024-05-27T21:01:26.275186Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\ndf_all = pd.concat([df_train, df_test], axis=0)\n\n# handle na value in numeric feature for dl\nnumeric_features = df_all.select_dtypes(include=np.number).columns\ndf_all[numeric_features] = df_all[numeric_features].apply(\n    lambda x: (x-x.mean())/(x.std()))\ndf_all[numeric_features] = df_all[numeric_features].fillna(0)\n\ntypes = df_all.dtypes\ncat_features = [col for col in df_all.columns if df_all[col].dtype.name == 'category' or df_all[col].dtype.name == 'object']\n\nfor col in df_all.columns:\n    if types[col] == 'object' or types[col] == 'category':\n#         print(col, df_all[col].nunique())\n        l_enc = LabelEncoder()\n        df_all[col] = l_enc.fit_transform(df_all[col].values)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-27T21:01:26.277541Z","iopub.execute_input":"2024-05-27T21:01:26.278124Z","iopub.status.idle":"2024-05-27T21:01:29.096208Z","shell.execute_reply.started":"2024-05-27T21:01:26.278097Z","shell.execute_reply":"2024-05-27T21:01:29.095364Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"df_all.replace([np.inf, -np.inf], np.nan, inplace=True)\ndf_all = df_all.dropna(axis=\"columns\")\n\ndf_train_d = df_all.head(len(df_train))\ndf_test_d = df_all.tail(len(df_test))\n\ndel df_all\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-05-27T21:01:29.097261Z","iopub.execute_input":"2024-05-27T21:01:29.097533Z","iopub.status.idle":"2024-05-27T21:01:29.494384Z","shell.execute_reply.started":"2024-05-27T21:01:29.097504Z","shell.execute_reply":"2024-05-27T21:01:29.493512Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nx_test_d = torch.tensor(df_test_d.values, dtype=torch.float32).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T21:01:29.495539Z","iopub.execute_input":"2024-05-27T21:01:29.495805Z","iopub.status.idle":"2024-05-27T21:01:29.686421Z","shell.execute_reply.started":"2024-05-27T21:01:29.495782Z","shell.execute_reply":"2024-05-27T21:01:29.685641Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"num_epoch, lr, batch_size, weight_decay = 5, 1e-3, 256, 1e-4\n\ndef get_model(model_name, x_train_d, device, batch_size):\n    # \"Transformer\", \"KAN\", \"TransformerKAN\"\n    if model_name == \"Transformer\":\n        print(\"Train Transformer\")\n        model = Transformer(in_features=len(x_train_d[0]), drop=0.).to(device)\n    elif model_name == \"KAN\":\n        print(\"Train KAN\")\n        model = KAN([len(x_train_d[0]), batch_size, 1]).to(device)\n    elif model_name == \"TransformerKAN\":\n        print(\"Train TransformerKAN\")\n        model = TransformerKAN(in_features=len(x_train_d[0]), drop=0.).to(device)\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-05-27T21:01:29.687548Z","iopub.execute_input":"2024-05-27T21:01:29.687824Z","iopub.status.idle":"2024-05-27T21:01:29.694304Z","shell.execute_reply.started":"2024-05-27T21:01:29.687801Z","shell.execute_reply":"2024-05-27T21:01:29.693459Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.metrics import roc_auc_score, auc\n\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier, LGBMRegressor\nimport catboost as cat\nfrom catboost import CatBoostClassifier, Pool\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nimport torch\nimport torch.nn as nn\nfrom torch.utils import data","metadata":{"execution":{"iopub.status.busy":"2024-05-27T21:01:29.695632Z","iopub.execute_input":"2024-05-27T21:01:29.696336Z","iopub.status.idle":"2024-05-27T21:01:30.001661Z","shell.execute_reply.started":"2024-05-27T21:01:29.696302Z","shell.execute_reply":"2024-05-27T21:01:30.000929Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"params = {\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"binary\",\n    \"metric\": \"auc\",\n    \"max_depth\": 10,  \n    \"learning_rate\": 0.05,\n    \"n_estimators\": 2000,  \n    \"colsample_bytree\": 0.8,\n    \"colsample_bynode\": 0.8,\n    \"verbose\": -1,\n    \"random_state\": 42,\n    \"reg_alpha\": 0.1,\n    \"reg_lambda\": 10,\n    \"extra_trees\":True,\n    'num_leaves':64,\n    \"device\": \"gpu\", \n    \"verbose\": 100,\n}","metadata":{"papermill":{"duration":0.017152,"end_time":"2024-04-12T05:04:00.172842","exception":false,"start_time":"2024-04-12T05:04:00.15569","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-27T21:01:30.002639Z","iopub.execute_input":"2024-05-27T21:01:30.002893Z","iopub.status.idle":"2024-05-27T21:01:30.007899Z","shell.execute_reply.started":"2024-05-27T21:01:30.002870Z","shell.execute_reply":"2024-05-27T21:01:30.007032Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"%%time\n\nfitted_models_cat = []\nfitted_models_lgb = []\nfitted_models_xgb = []\nfitted_models_transformer = []\nfitted_models_kan = []\nfitted_models_transformerkan = []\n\ncv_scores_cat = []\ncv_scores_lgb = []\ncv_scores_xgb = []\ncv_scores_transformer = []\ncv_scores_kan = []\ncv_scores_transformerkan = []\n\nfor idx_train, idx_valid in cv.split(df_train, y, groups=weeks):\n    X_train, y_train = df_train.iloc[idx_train], y.iloc[idx_train] \n    X_valid, y_valid = df_train.iloc[idx_valid], y.iloc[idx_valid]\n    train_pool = Pool(X_train, y_train, cat_features=cat_cols)\n    val_pool = Pool(X_valid, y_valid, cat_features=cat_cols)\n    clf = CatBoostClassifier(\n        eval_metric='AUC',\n        task_type='GPU',\n        learning_rate=0.03,\n        iterations=n_est,\n        random_seed=3107)\n    \n    clf.fit(train_pool, eval_set=val_pool,verbose=300)\n    fitted_models_cat.append(clf)\n    y_pred_valid = clf.predict_proba(X_valid)[:,1]\n    auc_score = roc_auc_score(y_valid, y_pred_valid)\n    cv_scores_cat.append(auc_score)\n    \n    \n    X_train[cat_cols] = X_train[cat_cols].astype(\"category\")\n    X_valid[cat_cols] = X_valid[cat_cols].astype(\"category\")\n    \n    model = lgb.LGBMClassifier(**params)\n    model.fit(\n        X_train, y_train,\n        eval_set = [(X_valid, y_valid)],\n        callbacks = [lgb.log_evaluation(200), lgb.early_stopping(100)])\n    \n    fitted_models_lgb.append(model)\n    y_pred_valid = model.predict_proba(X_valid)[:,1]\n    auc_score = roc_auc_score(y_valid, y_pred_valid)\n    cv_scores_lgb.append(auc_score)\n    \n    xgb_model = XGBClassifier(\n        device=\"cuda\",\n        objective='binary:logistic',\n        tree_method=\"hist\",\n        enable_categorical=True,\n        eval_metric='auc',\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_weight=1,\n        max_depth=20,\n        n_estimators=1200,\n        random_state=42,\n    )\n\n    # Training the model on the training data\n    xgb_model.fit(\n        X_train, y_train,\n        eval_set=[(X_valid, y_valid)],\n        early_stopping_rounds=100,\n        verbose=True,\n    )\n    fitted_models_xgb.append(xgb_model)\n    y_pred_valid = xgb_model.predict_proba(X_valid)[:,1]\n    auc_score = roc_auc_score(y_valid, y_pred_valid)\n    cv_scores_xgb.append(auc_score)\n    \nprint(\"CV AUC scores: \", cv_scores_cat)\nprint(\"Maximum CV AUC score: \", max(cv_scores_cat))\n\nprint(\"CV AUC scores: \", cv_scores_lgb)\nprint(\"Maximum CV AUC score: \", max(cv_scores_lgb))\n\nprint(\"CV AUC scores: \", cv_scores_xgb)\nprint(\"Maximum CV AUC score: \", max(cv_scores_xgb))\n\ndel df_train\ngc.collect()","metadata":{"papermill":{"duration":399.288009,"end_time":"2024-04-12T05:10:39.469766","exception":false,"start_time":"2024-04-12T05:04:00.181757","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-27T21:01:30.009063Z","iopub.execute_input":"2024-05-27T21:01:30.009315Z","iopub.status.idle":"2024-05-27T21:08:39.560144Z","shell.execute_reply.started":"2024-05-27T21:01:30.009293Z","shell.execute_reply":"2024-05-27T21:08:39.559247Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"Default metric period is 5 because AUC is/are not implemented for GPU\n","output_type":"stream"},{"name":"stdout","text":"0:\ttest: 0.5607415\tbest: 0.5607415 (0)\ttotal: 250ms\tremaining: 2m 29s\n300:\ttest: 0.7348446\tbest: 0.7348446 (300)\ttotal: 27.6s\tremaining: 27.4s\n599:\ttest: 0.7384720\tbest: 0.7385753 (590)\ttotal: 54.9s\tremaining: 0us\nbestTest = 0.7385753095\nbestIteration = 590\nShrink model to first 591 iterations.\n[LightGBM] [Info] Number of positive: 1397, number of negative: 38016\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.905121\n[LightGBM] [Info] Total Bins 29252\n[LightGBM] [Info] Number of data points in the train set: 39413, number of used features: 371\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","output_type":"stream"},{"name":"stderr","text":"1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 187 dense feature groups (7.07 MB) transferred to GPU in 0.006453 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035445 -> initscore=-3.303680\n[LightGBM] [Info] Start training from score -3.303680\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\nTraining until validation scores don't improve for 100 rounds\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 9\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 9\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 9\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 9\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 9\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 9\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 9\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 9\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 9\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Debug] Trained a tree with leaves = 56 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[200]\tvalid_0's auc: 0.73206\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Debug] Trained a tree with leaves = 60 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\nEarly stopping, best iteration is:\n[128]\tvalid_0's auc: 0.734203\n[0]\tvalidation_0-auc:0.49145\n[1]\tvalidation_0-auc:0.49052\n[2]\tvalidation_0-auc:0.51956\n[3]\tvalidation_0-auc:0.53496\n[4]\tvalidation_0-auc:0.55311\n[5]\tvalidation_0-auc:0.56393\n[6]\tvalidation_0-auc:0.57709\n[7]\tvalidation_0-auc:0.58313\n[8]\tvalidation_0-auc:0.59388\n[9]\tvalidation_0-auc:0.60395\n[10]\tvalidation_0-auc:0.61270\n[11]\tvalidation_0-auc:0.61886\n[12]\tvalidation_0-auc:0.62426\n[13]\tvalidation_0-auc:0.62736\n[14]\tvalidation_0-auc:0.63097\n[15]\tvalidation_0-auc:0.62843\n[16]\tvalidation_0-auc:0.62588\n[17]\tvalidation_0-auc:0.63175\n[18]\tvalidation_0-auc:0.63200\n[19]\tvalidation_0-auc:0.63396\n[20]\tvalidation_0-auc:0.63638\n[21]\tvalidation_0-auc:0.62985\n[22]\tvalidation_0-auc:0.63178\n[23]\tvalidation_0-auc:0.63246\n[24]\tvalidation_0-auc:0.63094\n[25]\tvalidation_0-auc:0.62935\n[26]\tvalidation_0-auc:0.63100\n[27]\tvalidation_0-auc:0.63351\n[28]\tvalidation_0-auc:0.63134\n[29]\tvalidation_0-auc:0.63394\n[30]\tvalidation_0-auc:0.63737\n[31]\tvalidation_0-auc:0.63871\n[32]\tvalidation_0-auc:0.63805\n[33]\tvalidation_0-auc:0.63697\n[34]\tvalidation_0-auc:0.63875\n[35]\tvalidation_0-auc:0.63773\n[36]\tvalidation_0-auc:0.63711\n[37]\tvalidation_0-auc:0.63770\n[38]\tvalidation_0-auc:0.63757\n[39]\tvalidation_0-auc:0.63937\n[40]\tvalidation_0-auc:0.63987\n[41]\tvalidation_0-auc:0.64008\n[42]\tvalidation_0-auc:0.64184\n[43]\tvalidation_0-auc:0.64256\n[44]\tvalidation_0-auc:0.64351\n[45]\tvalidation_0-auc:0.64454\n[46]\tvalidation_0-auc:0.64441\n[47]\tvalidation_0-auc:0.64511\n[48]\tvalidation_0-auc:0.64613\n[49]\tvalidation_0-auc:0.64545\n[50]\tvalidation_0-auc:0.64619\n[51]\tvalidation_0-auc:0.64573\n[52]\tvalidation_0-auc:0.64514\n[53]\tvalidation_0-auc:0.64724\n[54]\tvalidation_0-auc:0.64762\n[55]\tvalidation_0-auc:0.64900\n[56]\tvalidation_0-auc:0.64728\n[57]\tvalidation_0-auc:0.64598\n[58]\tvalidation_0-auc:0.64673\n[59]\tvalidation_0-auc:0.64804\n[60]\tvalidation_0-auc:0.64902\n[61]\tvalidation_0-auc:0.65027\n[62]\tvalidation_0-auc:0.65126\n[63]\tvalidation_0-auc:0.65079\n[64]\tvalidation_0-auc:0.64984\n[65]\tvalidation_0-auc:0.65071\n[66]\tvalidation_0-auc:0.65147\n[67]\tvalidation_0-auc:0.65201\n[68]\tvalidation_0-auc:0.65226\n[69]\tvalidation_0-auc:0.65297\n[70]\tvalidation_0-auc:0.65378\n[71]\tvalidation_0-auc:0.65442\n[72]\tvalidation_0-auc:0.65435\n[73]\tvalidation_0-auc:0.65478\n[74]\tvalidation_0-auc:0.65561\n[75]\tvalidation_0-auc:0.65633\n[76]\tvalidation_0-auc:0.65698\n[77]\tvalidation_0-auc:0.65746\n[78]\tvalidation_0-auc:0.65936\n[79]\tvalidation_0-auc:0.65874\n[80]\tvalidation_0-auc:0.65913\n[81]\tvalidation_0-auc:0.65821\n[82]\tvalidation_0-auc:0.65869\n[83]\tvalidation_0-auc:0.65862\n[84]\tvalidation_0-auc:0.65877\n[85]\tvalidation_0-auc:0.65948\n[86]\tvalidation_0-auc:0.65865\n[87]\tvalidation_0-auc:0.65917\n[88]\tvalidation_0-auc:0.65880\n[89]\tvalidation_0-auc:0.65849\n[90]\tvalidation_0-auc:0.65863\n[91]\tvalidation_0-auc:0.65893\n[92]\tvalidation_0-auc:0.65931\n[93]\tvalidation_0-auc:0.65930\n[94]\tvalidation_0-auc:0.65980\n[95]\tvalidation_0-auc:0.65919\n[96]\tvalidation_0-auc:0.65928\n[97]\tvalidation_0-auc:0.65990\n[98]\tvalidation_0-auc:0.65933\n[99]\tvalidation_0-auc:0.65948\n[100]\tvalidation_0-auc:0.65896\n[101]\tvalidation_0-auc:0.65922\n[102]\tvalidation_0-auc:0.65922\n[103]\tvalidation_0-auc:0.65963\n[104]\tvalidation_0-auc:0.65985\n[105]\tvalidation_0-auc:0.65991\n[106]\tvalidation_0-auc:0.66076\n[107]\tvalidation_0-auc:0.66044\n[108]\tvalidation_0-auc:0.66139\n[109]\tvalidation_0-auc:0.66072\n[110]\tvalidation_0-auc:0.66073\n[111]\tvalidation_0-auc:0.66134\n[112]\tvalidation_0-auc:0.66104\n[113]\tvalidation_0-auc:0.66155\n[114]\tvalidation_0-auc:0.66209\n[115]\tvalidation_0-auc:0.66191\n[116]\tvalidation_0-auc:0.66300\n[117]\tvalidation_0-auc:0.66315\n[118]\tvalidation_0-auc:0.66247\n[119]\tvalidation_0-auc:0.66208\n[120]\tvalidation_0-auc:0.66201\n[121]\tvalidation_0-auc:0.66216\n[122]\tvalidation_0-auc:0.66285\n[123]\tvalidation_0-auc:0.66311\n[124]\tvalidation_0-auc:0.66313\n[125]\tvalidation_0-auc:0.66359\n[126]\tvalidation_0-auc:0.66393\n[127]\tvalidation_0-auc:0.66365\n[128]\tvalidation_0-auc:0.66342\n[129]\tvalidation_0-auc:0.66322\n[130]\tvalidation_0-auc:0.66306\n[131]\tvalidation_0-auc:0.66372\n[132]\tvalidation_0-auc:0.66430\n[133]\tvalidation_0-auc:0.66370\n[134]\tvalidation_0-auc:0.66371\n[135]\tvalidation_0-auc:0.66360\n[136]\tvalidation_0-auc:0.66377\n[137]\tvalidation_0-auc:0.66471\n[138]\tvalidation_0-auc:0.66470\n[139]\tvalidation_0-auc:0.66405\n[140]\tvalidation_0-auc:0.66473\n[141]\tvalidation_0-auc:0.66415\n[142]\tvalidation_0-auc:0.66388\n[143]\tvalidation_0-auc:0.66325\n[144]\tvalidation_0-auc:0.66257\n[145]\tvalidation_0-auc:0.66293\n[146]\tvalidation_0-auc:0.66285\n[147]\tvalidation_0-auc:0.66219\n[148]\tvalidation_0-auc:0.66236\n[149]\tvalidation_0-auc:0.66300\n[150]\tvalidation_0-auc:0.66281\n[151]\tvalidation_0-auc:0.66273\n[152]\tvalidation_0-auc:0.66241\n[153]\tvalidation_0-auc:0.66254\n[154]\tvalidation_0-auc:0.66314\n[155]\tvalidation_0-auc:0.66355\n[156]\tvalidation_0-auc:0.66299\n[157]\tvalidation_0-auc:0.66311\n[158]\tvalidation_0-auc:0.66331\n[159]\tvalidation_0-auc:0.66313\n[160]\tvalidation_0-auc:0.66325\n[161]\tvalidation_0-auc:0.66355\n[162]\tvalidation_0-auc:0.66355\n[163]\tvalidation_0-auc:0.66346\n[164]\tvalidation_0-auc:0.66302\n[165]\tvalidation_0-auc:0.66298\n[166]\tvalidation_0-auc:0.66255\n[167]\tvalidation_0-auc:0.66290\n[168]\tvalidation_0-auc:0.66250\n[169]\tvalidation_0-auc:0.66259\n[170]\tvalidation_0-auc:0.66245\n[171]\tvalidation_0-auc:0.66217\n[172]\tvalidation_0-auc:0.66233\n[173]\tvalidation_0-auc:0.66302\n[174]\tvalidation_0-auc:0.66248\n[175]\tvalidation_0-auc:0.66260\n[176]\tvalidation_0-auc:0.66209\n[177]\tvalidation_0-auc:0.66219\n[178]\tvalidation_0-auc:0.66238\n[179]\tvalidation_0-auc:0.66263\n[180]\tvalidation_0-auc:0.66283\n[181]\tvalidation_0-auc:0.66266\n[182]\tvalidation_0-auc:0.66227\n[183]\tvalidation_0-auc:0.66267\n[184]\tvalidation_0-auc:0.66291\n[185]\tvalidation_0-auc:0.66338\n[186]\tvalidation_0-auc:0.66326\n[187]\tvalidation_0-auc:0.66272\n[188]\tvalidation_0-auc:0.66312\n[189]\tvalidation_0-auc:0.66334\n[190]\tvalidation_0-auc:0.66280\n[191]\tvalidation_0-auc:0.66320\n[192]\tvalidation_0-auc:0.66310\n[193]\tvalidation_0-auc:0.66344\n[194]\tvalidation_0-auc:0.66319\n[195]\tvalidation_0-auc:0.66335\n[196]\tvalidation_0-auc:0.66338\n[197]\tvalidation_0-auc:0.66299\n[198]\tvalidation_0-auc:0.66310\n[199]\tvalidation_0-auc:0.66285\n[200]\tvalidation_0-auc:0.66302\n[201]\tvalidation_0-auc:0.66305\n[202]\tvalidation_0-auc:0.66362\n[203]\tvalidation_0-auc:0.66309\n[204]\tvalidation_0-auc:0.66311\n[205]\tvalidation_0-auc:0.66350\n[206]\tvalidation_0-auc:0.66314\n[207]\tvalidation_0-auc:0.66309\n[208]\tvalidation_0-auc:0.66279\n[209]\tvalidation_0-auc:0.66247\n[210]\tvalidation_0-auc:0.66273\n[211]\tvalidation_0-auc:0.66267\n[212]\tvalidation_0-auc:0.66279\n[213]\tvalidation_0-auc:0.66246\n[214]\tvalidation_0-auc:0.66270\n[215]\tvalidation_0-auc:0.66199\n[216]\tvalidation_0-auc:0.66205\n[217]\tvalidation_0-auc:0.66236\n[218]\tvalidation_0-auc:0.66244\n[219]\tvalidation_0-auc:0.66277\n[220]\tvalidation_0-auc:0.66251\n[221]\tvalidation_0-auc:0.66238\n[222]\tvalidation_0-auc:0.66238\n[223]\tvalidation_0-auc:0.66255\n[224]\tvalidation_0-auc:0.66257\n[225]\tvalidation_0-auc:0.66304\n[226]\tvalidation_0-auc:0.66280\n[227]\tvalidation_0-auc:0.66247\n[228]\tvalidation_0-auc:0.66294\n[229]\tvalidation_0-auc:0.66352\n[230]\tvalidation_0-auc:0.66300\n[231]\tvalidation_0-auc:0.66363\n[232]\tvalidation_0-auc:0.66366\n[233]\tvalidation_0-auc:0.66378\n[234]\tvalidation_0-auc:0.66376\n[235]\tvalidation_0-auc:0.66408\n[236]\tvalidation_0-auc:0.66390\n[237]\tvalidation_0-auc:0.66411\n[238]\tvalidation_0-auc:0.66419\n[239]\tvalidation_0-auc:0.66459\n[240]\tvalidation_0-auc:0.66421\n","output_type":"stream"},{"name":"stderr","text":"Default metric period is 5 because AUC is/are not implemented for GPU\n","output_type":"stream"},{"name":"stdout","text":"0:\ttest: 0.6395709\tbest: 0.6395709 (0)\ttotal: 139ms\tremaining: 1m 22s\n300:\ttest: 0.7363773\tbest: 0.7363773 (300)\ttotal: 28.1s\tremaining: 27.9s\n599:\ttest: 0.7393854\tbest: 0.7393854 (599)\ttotal: 55.8s\tremaining: 0us\nbestTest = 0.739385426\nbestIteration = 599\n[LightGBM] [Info] Number of positive: 1382, number of negative: 39058\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.905649\n[LightGBM] [Info] Total Bins 29111\n[LightGBM] [Info] Number of data points in the train set: 40440, number of used features: 373\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 185 dense feature groups (7.25 MB) transferred to GPU in 0.006136 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.034174 -> initscore=-3.341516\n[LightGBM] [Info] Start training from score -3.341516\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\nTraining until validation scores don't improve for 100 rounds\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 9\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 9\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 9\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 9\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 9\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 9\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 9\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 9\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 9\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Debug] Trained a tree with leaves = 53 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\nEarly stopping, best iteration is:\n[82]\tvalid_0's auc: 0.748946\n[0]\tvalidation_0-auc:0.54893\n[1]\tvalidation_0-auc:0.50500\n[2]\tvalidation_0-auc:0.52174\n[3]\tvalidation_0-auc:0.55694\n[4]\tvalidation_0-auc:0.58152\n[5]\tvalidation_0-auc:0.60562\n[6]\tvalidation_0-auc:0.62229\n[7]\tvalidation_0-auc:0.62782\n[8]\tvalidation_0-auc:0.63503\n[9]\tvalidation_0-auc:0.63784\n[10]\tvalidation_0-auc:0.63880\n[11]\tvalidation_0-auc:0.63373\n[12]\tvalidation_0-auc:0.63979\n[13]\tvalidation_0-auc:0.64398\n[14]\tvalidation_0-auc:0.65363\n[15]\tvalidation_0-auc:0.65854\n[16]\tvalidation_0-auc:0.65685\n[17]\tvalidation_0-auc:0.65064\n[18]\tvalidation_0-auc:0.64972\n[19]\tvalidation_0-auc:0.65245\n[20]\tvalidation_0-auc:0.65367\n[21]\tvalidation_0-auc:0.65268\n[22]\tvalidation_0-auc:0.65666\n[23]\tvalidation_0-auc:0.65457\n[24]\tvalidation_0-auc:0.65407\n[25]\tvalidation_0-auc:0.65636\n[26]\tvalidation_0-auc:0.65704\n[27]\tvalidation_0-auc:0.65924\n[28]\tvalidation_0-auc:0.66107\n[29]\tvalidation_0-auc:0.66126\n[30]\tvalidation_0-auc:0.66296\n[31]\tvalidation_0-auc:0.66277\n[32]\tvalidation_0-auc:0.66259\n[33]\tvalidation_0-auc:0.66153\n[34]\tvalidation_0-auc:0.66228\n[35]\tvalidation_0-auc:0.66389\n[36]\tvalidation_0-auc:0.66366\n[37]\tvalidation_0-auc:0.66420\n[38]\tvalidation_0-auc:0.66533\n[39]\tvalidation_0-auc:0.66668\n[40]\tvalidation_0-auc:0.66816\n[41]\tvalidation_0-auc:0.66941\n[42]\tvalidation_0-auc:0.67138\n[43]\tvalidation_0-auc:0.67144\n[44]\tvalidation_0-auc:0.67218\n[45]\tvalidation_0-auc:0.67217\n[46]\tvalidation_0-auc:0.67122\n[47]\tvalidation_0-auc:0.67245\n[48]\tvalidation_0-auc:0.67334\n[49]\tvalidation_0-auc:0.67492\n[50]\tvalidation_0-auc:0.67581\n[51]\tvalidation_0-auc:0.67597\n[52]\tvalidation_0-auc:0.67562\n[53]\tvalidation_0-auc:0.67527\n[54]\tvalidation_0-auc:0.67441\n[55]\tvalidation_0-auc:0.67527\n[56]\tvalidation_0-auc:0.67544\n[57]\tvalidation_0-auc:0.67503\n[58]\tvalidation_0-auc:0.67660\n[59]\tvalidation_0-auc:0.67737\n[60]\tvalidation_0-auc:0.67787\n[61]\tvalidation_0-auc:0.67762\n[62]\tvalidation_0-auc:0.67882\n[63]\tvalidation_0-auc:0.67800\n[64]\tvalidation_0-auc:0.67912\n[65]\tvalidation_0-auc:0.68002\n[66]\tvalidation_0-auc:0.68076\n[67]\tvalidation_0-auc:0.67984\n[68]\tvalidation_0-auc:0.67946\n[69]\tvalidation_0-auc:0.67884\n[70]\tvalidation_0-auc:0.68007\n[71]\tvalidation_0-auc:0.68185\n[72]\tvalidation_0-auc:0.68326\n[73]\tvalidation_0-auc:0.68309\n[74]\tvalidation_0-auc:0.68313\n[75]\tvalidation_0-auc:0.68335\n[76]\tvalidation_0-auc:0.68312\n[77]\tvalidation_0-auc:0.68483\n[78]\tvalidation_0-auc:0.68505\n[79]\tvalidation_0-auc:0.68562\n[80]\tvalidation_0-auc:0.68503\n[81]\tvalidation_0-auc:0.68562\n[82]\tvalidation_0-auc:0.68615\n[83]\tvalidation_0-auc:0.68606\n[84]\tvalidation_0-auc:0.68659\n[85]\tvalidation_0-auc:0.68715\n[86]\tvalidation_0-auc:0.68664\n[87]\tvalidation_0-auc:0.68726\n[88]\tvalidation_0-auc:0.68715\n[89]\tvalidation_0-auc:0.68720\n[90]\tvalidation_0-auc:0.68624\n[91]\tvalidation_0-auc:0.68690\n[92]\tvalidation_0-auc:0.68676\n[93]\tvalidation_0-auc:0.68689\n[94]\tvalidation_0-auc:0.68726\n[95]\tvalidation_0-auc:0.68730\n[96]\tvalidation_0-auc:0.68727\n[97]\tvalidation_0-auc:0.68736\n[98]\tvalidation_0-auc:0.68734\n[99]\tvalidation_0-auc:0.68847\n[100]\tvalidation_0-auc:0.68870\n[101]\tvalidation_0-auc:0.68932\n[102]\tvalidation_0-auc:0.68988\n[103]\tvalidation_0-auc:0.69039\n[104]\tvalidation_0-auc:0.68959\n[105]\tvalidation_0-auc:0.68891\n[106]\tvalidation_0-auc:0.68913\n[107]\tvalidation_0-auc:0.68945\n[108]\tvalidation_0-auc:0.69020\n[109]\tvalidation_0-auc:0.69118\n[110]\tvalidation_0-auc:0.69149\n[111]\tvalidation_0-auc:0.69087\n[112]\tvalidation_0-auc:0.69113\n[113]\tvalidation_0-auc:0.69144\n[114]\tvalidation_0-auc:0.69127\n[115]\tvalidation_0-auc:0.69113\n[116]\tvalidation_0-auc:0.69107\n[117]\tvalidation_0-auc:0.69102\n[118]\tvalidation_0-auc:0.69097\n[119]\tvalidation_0-auc:0.69083\n[120]\tvalidation_0-auc:0.69101\n[121]\tvalidation_0-auc:0.69028\n[122]\tvalidation_0-auc:0.69051\n[123]\tvalidation_0-auc:0.69097\n[124]\tvalidation_0-auc:0.69059\n[125]\tvalidation_0-auc:0.68977\n[126]\tvalidation_0-auc:0.69034\n[127]\tvalidation_0-auc:0.69048\n[128]\tvalidation_0-auc:0.69140\n[129]\tvalidation_0-auc:0.69177\n[130]\tvalidation_0-auc:0.69111\n[131]\tvalidation_0-auc:0.69217\n[132]\tvalidation_0-auc:0.69150\n[133]\tvalidation_0-auc:0.69173\n[134]\tvalidation_0-auc:0.69158\n[135]\tvalidation_0-auc:0.69140\n[136]\tvalidation_0-auc:0.69079\n[137]\tvalidation_0-auc:0.69137\n[138]\tvalidation_0-auc:0.69123\n[139]\tvalidation_0-auc:0.69148\n[140]\tvalidation_0-auc:0.69193\n[141]\tvalidation_0-auc:0.69135\n[142]\tvalidation_0-auc:0.69151\n[143]\tvalidation_0-auc:0.69135\n[144]\tvalidation_0-auc:0.69111\n[145]\tvalidation_0-auc:0.69062\n[146]\tvalidation_0-auc:0.69074\n[147]\tvalidation_0-auc:0.69078\n[148]\tvalidation_0-auc:0.69078\n[149]\tvalidation_0-auc:0.69081\n[150]\tvalidation_0-auc:0.69091\n[151]\tvalidation_0-auc:0.69056\n[152]\tvalidation_0-auc:0.69131\n[153]\tvalidation_0-auc:0.69175\n[154]\tvalidation_0-auc:0.69143\n[155]\tvalidation_0-auc:0.69169\n[156]\tvalidation_0-auc:0.69186\n[157]\tvalidation_0-auc:0.69236\n[158]\tvalidation_0-auc:0.69223\n[159]\tvalidation_0-auc:0.69273\n[160]\tvalidation_0-auc:0.69303\n[161]\tvalidation_0-auc:0.69347\n[162]\tvalidation_0-auc:0.69355\n[163]\tvalidation_0-auc:0.69418\n[164]\tvalidation_0-auc:0.69491\n[165]\tvalidation_0-auc:0.69483\n[166]\tvalidation_0-auc:0.69520\n[167]\tvalidation_0-auc:0.69553\n[168]\tvalidation_0-auc:0.69560\n[169]\tvalidation_0-auc:0.69539\n[170]\tvalidation_0-auc:0.69514\n[171]\tvalidation_0-auc:0.69450\n[172]\tvalidation_0-auc:0.69416\n[173]\tvalidation_0-auc:0.69375\n[174]\tvalidation_0-auc:0.69432\n[175]\tvalidation_0-auc:0.69491\n[176]\tvalidation_0-auc:0.69515\n[177]\tvalidation_0-auc:0.69558\n[178]\tvalidation_0-auc:0.69609\n[179]\tvalidation_0-auc:0.69544\n[180]\tvalidation_0-auc:0.69541\n[181]\tvalidation_0-auc:0.69594\n[182]\tvalidation_0-auc:0.69583\n[183]\tvalidation_0-auc:0.69626\n[184]\tvalidation_0-auc:0.69559\n[185]\tvalidation_0-auc:0.69628\n[186]\tvalidation_0-auc:0.69645\n[187]\tvalidation_0-auc:0.69664\n[188]\tvalidation_0-auc:0.69663\n[189]\tvalidation_0-auc:0.69655\n[190]\tvalidation_0-auc:0.69599\n[191]\tvalidation_0-auc:0.69580\n[192]\tvalidation_0-auc:0.69654\n[193]\tvalidation_0-auc:0.69669\n[194]\tvalidation_0-auc:0.69634\n[195]\tvalidation_0-auc:0.69615\n[196]\tvalidation_0-auc:0.69655\n[197]\tvalidation_0-auc:0.69673\n[198]\tvalidation_0-auc:0.69622\n[199]\tvalidation_0-auc:0.69637\n[200]\tvalidation_0-auc:0.69608\n[201]\tvalidation_0-auc:0.69561\n[202]\tvalidation_0-auc:0.69538\n[203]\tvalidation_0-auc:0.69518\n[204]\tvalidation_0-auc:0.69516\n[205]\tvalidation_0-auc:0.69535\n[206]\tvalidation_0-auc:0.69518\n[207]\tvalidation_0-auc:0.69548\n[208]\tvalidation_0-auc:0.69579\n[209]\tvalidation_0-auc:0.69585\n[210]\tvalidation_0-auc:0.69584\n[211]\tvalidation_0-auc:0.69549\n[212]\tvalidation_0-auc:0.69523\n[213]\tvalidation_0-auc:0.69491\n[214]\tvalidation_0-auc:0.69489\n[215]\tvalidation_0-auc:0.69504\n[216]\tvalidation_0-auc:0.69433\n[217]\tvalidation_0-auc:0.69382\n[218]\tvalidation_0-auc:0.69395\n[219]\tvalidation_0-auc:0.69413\n[220]\tvalidation_0-auc:0.69406\n[221]\tvalidation_0-auc:0.69403\n[222]\tvalidation_0-auc:0.69415\n[223]\tvalidation_0-auc:0.69423\n[224]\tvalidation_0-auc:0.69448\n[225]\tvalidation_0-auc:0.69469\n[226]\tvalidation_0-auc:0.69481\n[227]\tvalidation_0-auc:0.69418\n[228]\tvalidation_0-auc:0.69360\n[229]\tvalidation_0-auc:0.69398\n[230]\tvalidation_0-auc:0.69374\n[231]\tvalidation_0-auc:0.69416\n[232]\tvalidation_0-auc:0.69385\n[233]\tvalidation_0-auc:0.69423\n[234]\tvalidation_0-auc:0.69449\n[235]\tvalidation_0-auc:0.69442\n[236]\tvalidation_0-auc:0.69469\n[237]\tvalidation_0-auc:0.69487\n[238]\tvalidation_0-auc:0.69490\n[239]\tvalidation_0-auc:0.69492\n[240]\tvalidation_0-auc:0.69451\n[241]\tvalidation_0-auc:0.69415\n[242]\tvalidation_0-auc:0.69420\n[243]\tvalidation_0-auc:0.69391\n[244]\tvalidation_0-auc:0.69323\n[245]\tvalidation_0-auc:0.69300\n[246]\tvalidation_0-auc:0.69306\n[247]\tvalidation_0-auc:0.69297\n[248]\tvalidation_0-auc:0.69281\n[249]\tvalidation_0-auc:0.69255\n[250]\tvalidation_0-auc:0.69214\n[251]\tvalidation_0-auc:0.69232\n[252]\tvalidation_0-auc:0.69228\n[253]\tvalidation_0-auc:0.69242\n[254]\tvalidation_0-auc:0.69263\n[255]\tvalidation_0-auc:0.69320\n[256]\tvalidation_0-auc:0.69303\n[257]\tvalidation_0-auc:0.69317\n[258]\tvalidation_0-auc:0.69348\n[259]\tvalidation_0-auc:0.69376\n[260]\tvalidation_0-auc:0.69380\n[261]\tvalidation_0-auc:0.69399\n[262]\tvalidation_0-auc:0.69401\n[263]\tvalidation_0-auc:0.69399\n[264]\tvalidation_0-auc:0.69393\n[265]\tvalidation_0-auc:0.69334\n[266]\tvalidation_0-auc:0.69364\n[267]\tvalidation_0-auc:0.69349\n[268]\tvalidation_0-auc:0.69344\n[269]\tvalidation_0-auc:0.69353\n[270]\tvalidation_0-auc:0.69367\n[271]\tvalidation_0-auc:0.69381\n[272]\tvalidation_0-auc:0.69422\n[273]\tvalidation_0-auc:0.69448\n[274]\tvalidation_0-auc:0.69471\n[275]\tvalidation_0-auc:0.69507\n[276]\tvalidation_0-auc:0.69530\n[277]\tvalidation_0-auc:0.69526\n[278]\tvalidation_0-auc:0.69464\n[279]\tvalidation_0-auc:0.69453\n[280]\tvalidation_0-auc:0.69435\n[281]\tvalidation_0-auc:0.69474\n[282]\tvalidation_0-auc:0.69463\n[283]\tvalidation_0-auc:0.69450\n[284]\tvalidation_0-auc:0.69480\n[285]\tvalidation_0-auc:0.69466\n[286]\tvalidation_0-auc:0.69500\n[287]\tvalidation_0-auc:0.69506\n[288]\tvalidation_0-auc:0.69494\n[289]\tvalidation_0-auc:0.69461\n[290]\tvalidation_0-auc:0.69468\n[291]\tvalidation_0-auc:0.69440\n[292]\tvalidation_0-auc:0.69422\n[293]\tvalidation_0-auc:0.69386\n[294]\tvalidation_0-auc:0.69362\n[295]\tvalidation_0-auc:0.69356\n[296]\tvalidation_0-auc:0.69384\n[297]\tvalidation_0-auc:0.69406\n","output_type":"stream"},{"name":"stderr","text":"Default metric period is 5 because AUC is/are not implemented for GPU\n","output_type":"stream"},{"name":"stdout","text":"0:\ttest: 0.6456748\tbest: 0.6456748 (0)\ttotal: 143ms\tremaining: 1m 25s\n300:\ttest: 0.7647312\tbest: 0.7647312 (300)\ttotal: 28.3s\tremaining: 28.1s\n599:\ttest: 0.7717138\tbest: 0.7717138 (599)\ttotal: 56.7s\tremaining: 0us\nbestTest = 0.7717137933\nbestIteration = 599\n[LightGBM] [Info] Number of positive: 1393, number of negative: 38750\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.903605\n[LightGBM] [Info] Total Bins 29101\n[LightGBM] [Info] Number of data points in the train set: 40143, number of used features: 373\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 186 dense feature groups (7.20 MB) transferred to GPU in 0.006109 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.034701 -> initscore=-3.325671\n[LightGBM] [Info] Start training from score -3.325671\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 9\nTraining until validation scores don't improve for 100 rounds\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 9\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 9\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 9\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Debug] Trained a tree with leaves = 47 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Debug] Trained a tree with leaves = 55 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[200]\tvalid_0's auc: 0.785957\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Debug] Trained a tree with leaves = 61 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Debug] Trained a tree with leaves = 53 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Debug] Trained a tree with leaves = 59 and depth = 10\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Debug] Trained a tree with leaves = 63 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Debug] Trained a tree with leaves = 48 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\nEarly stopping, best iteration is:\n[142]\tvalid_0's auc: 0.787066\n[0]\tvalidation_0-auc:0.55821\n[1]\tvalidation_0-auc:0.56307\n[2]\tvalidation_0-auc:0.58442\n[3]\tvalidation_0-auc:0.61907\n[4]\tvalidation_0-auc:0.62412\n[5]\tvalidation_0-auc:0.62133\n[6]\tvalidation_0-auc:0.61648\n[7]\tvalidation_0-auc:0.61748\n[8]\tvalidation_0-auc:0.62313\n[9]\tvalidation_0-auc:0.64577\n[10]\tvalidation_0-auc:0.65331\n[11]\tvalidation_0-auc:0.65329\n[12]\tvalidation_0-auc:0.65336\n[13]\tvalidation_0-auc:0.65475\n[14]\tvalidation_0-auc:0.65444\n[15]\tvalidation_0-auc:0.66136\n[16]\tvalidation_0-auc:0.66452\n[17]\tvalidation_0-auc:0.66546\n[18]\tvalidation_0-auc:0.66614\n[19]\tvalidation_0-auc:0.66183\n[20]\tvalidation_0-auc:0.66261\n[21]\tvalidation_0-auc:0.65876\n[22]\tvalidation_0-auc:0.65860\n[23]\tvalidation_0-auc:0.65874\n[24]\tvalidation_0-auc:0.66215\n[25]\tvalidation_0-auc:0.66219\n[26]\tvalidation_0-auc:0.66495\n[27]\tvalidation_0-auc:0.66716\n[28]\tvalidation_0-auc:0.66703\n[29]\tvalidation_0-auc:0.66551\n[30]\tvalidation_0-auc:0.66997\n[31]\tvalidation_0-auc:0.67155\n[32]\tvalidation_0-auc:0.66980\n[33]\tvalidation_0-auc:0.66995\n[34]\tvalidation_0-auc:0.67075\n[35]\tvalidation_0-auc:0.66764\n[36]\tvalidation_0-auc:0.66812\n[37]\tvalidation_0-auc:0.66828\n[38]\tvalidation_0-auc:0.67120\n[39]\tvalidation_0-auc:0.67022\n[40]\tvalidation_0-auc:0.67197\n[41]\tvalidation_0-auc:0.67194\n[42]\tvalidation_0-auc:0.67284\n[43]\tvalidation_0-auc:0.67342\n[44]\tvalidation_0-auc:0.67384\n[45]\tvalidation_0-auc:0.67611\n[46]\tvalidation_0-auc:0.67600\n[47]\tvalidation_0-auc:0.67582\n[48]\tvalidation_0-auc:0.67497\n[49]\tvalidation_0-auc:0.67597\n[50]\tvalidation_0-auc:0.67444\n[51]\tvalidation_0-auc:0.67535\n[52]\tvalidation_0-auc:0.67487\n[53]\tvalidation_0-auc:0.67579\n[54]\tvalidation_0-auc:0.67536\n[55]\tvalidation_0-auc:0.67503\n[56]\tvalidation_0-auc:0.67475\n[57]\tvalidation_0-auc:0.67452\n[58]\tvalidation_0-auc:0.67434\n[59]\tvalidation_0-auc:0.67667\n[60]\tvalidation_0-auc:0.67595\n[61]\tvalidation_0-auc:0.67562\n[62]\tvalidation_0-auc:0.67684\n[63]\tvalidation_0-auc:0.67673\n[64]\tvalidation_0-auc:0.67376\n[65]\tvalidation_0-auc:0.67463\n[66]\tvalidation_0-auc:0.67514\n[67]\tvalidation_0-auc:0.67428\n[68]\tvalidation_0-auc:0.67534\n[69]\tvalidation_0-auc:0.67507\n[70]\tvalidation_0-auc:0.67533\n[71]\tvalidation_0-auc:0.67550\n[72]\tvalidation_0-auc:0.67126\n[73]\tvalidation_0-auc:0.67027\n[74]\tvalidation_0-auc:0.67051\n[75]\tvalidation_0-auc:0.67095\n[76]\tvalidation_0-auc:0.67102\n[77]\tvalidation_0-auc:0.67045\n[78]\tvalidation_0-auc:0.67137\n[79]\tvalidation_0-auc:0.67261\n[80]\tvalidation_0-auc:0.67106\n[81]\tvalidation_0-auc:0.67291\n[82]\tvalidation_0-auc:0.67310\n[83]\tvalidation_0-auc:0.67457\n[84]\tvalidation_0-auc:0.67452\n[85]\tvalidation_0-auc:0.67466\n[86]\tvalidation_0-auc:0.67336\n[87]\tvalidation_0-auc:0.67338\n[88]\tvalidation_0-auc:0.67263\n[89]\tvalidation_0-auc:0.67273\n[90]\tvalidation_0-auc:0.67258\n[91]\tvalidation_0-auc:0.67262\n[92]\tvalidation_0-auc:0.67373\n[93]\tvalidation_0-auc:0.67249\n[94]\tvalidation_0-auc:0.67248\n[95]\tvalidation_0-auc:0.67253\n[96]\tvalidation_0-auc:0.67290\n[97]\tvalidation_0-auc:0.67285\n[98]\tvalidation_0-auc:0.67428\n[99]\tvalidation_0-auc:0.67443\n[100]\tvalidation_0-auc:0.67460\n[101]\tvalidation_0-auc:0.67483\n[102]\tvalidation_0-auc:0.67585\n[103]\tvalidation_0-auc:0.67625\n[104]\tvalidation_0-auc:0.67721\n[105]\tvalidation_0-auc:0.67785\n[106]\tvalidation_0-auc:0.67806\n[107]\tvalidation_0-auc:0.67797\n[108]\tvalidation_0-auc:0.67862\n[109]\tvalidation_0-auc:0.67817\n[110]\tvalidation_0-auc:0.67867\n[111]\tvalidation_0-auc:0.67853\n[112]\tvalidation_0-auc:0.67848\n[113]\tvalidation_0-auc:0.67928\n[114]\tvalidation_0-auc:0.67902\n[115]\tvalidation_0-auc:0.67953\n[116]\tvalidation_0-auc:0.67965\n[117]\tvalidation_0-auc:0.67986\n[118]\tvalidation_0-auc:0.67872\n[119]\tvalidation_0-auc:0.67973\n[120]\tvalidation_0-auc:0.67954\n[121]\tvalidation_0-auc:0.68017\n[122]\tvalidation_0-auc:0.68049\n[123]\tvalidation_0-auc:0.68031\n[124]\tvalidation_0-auc:0.68084\n[125]\tvalidation_0-auc:0.68039\n[126]\tvalidation_0-auc:0.68087\n[127]\tvalidation_0-auc:0.68064\n[128]\tvalidation_0-auc:0.68120\n[129]\tvalidation_0-auc:0.68215\n[130]\tvalidation_0-auc:0.68152\n[131]\tvalidation_0-auc:0.68183\n[132]\tvalidation_0-auc:0.68201\n[133]\tvalidation_0-auc:0.68235\n[134]\tvalidation_0-auc:0.68248\n[135]\tvalidation_0-auc:0.68227\n[136]\tvalidation_0-auc:0.68271\n[137]\tvalidation_0-auc:0.68295\n[138]\tvalidation_0-auc:0.68253\n[139]\tvalidation_0-auc:0.68306\n[140]\tvalidation_0-auc:0.68249\n[141]\tvalidation_0-auc:0.68207\n[142]\tvalidation_0-auc:0.68155\n[143]\tvalidation_0-auc:0.68194\n[144]\tvalidation_0-auc:0.68231\n[145]\tvalidation_0-auc:0.68273\n[146]\tvalidation_0-auc:0.68325\n[147]\tvalidation_0-auc:0.68331\n[148]\tvalidation_0-auc:0.68295\n[149]\tvalidation_0-auc:0.68342\n[150]\tvalidation_0-auc:0.68350\n[151]\tvalidation_0-auc:0.68428\n[152]\tvalidation_0-auc:0.68375\n[153]\tvalidation_0-auc:0.68368\n[154]\tvalidation_0-auc:0.68360\n[155]\tvalidation_0-auc:0.68329\n[156]\tvalidation_0-auc:0.68360\n[157]\tvalidation_0-auc:0.68421\n[158]\tvalidation_0-auc:0.68414\n[159]\tvalidation_0-auc:0.68373\n[160]\tvalidation_0-auc:0.68322\n[161]\tvalidation_0-auc:0.68336\n[162]\tvalidation_0-auc:0.68340\n[163]\tvalidation_0-auc:0.68289\n[164]\tvalidation_0-auc:0.68244\n[165]\tvalidation_0-auc:0.68107\n[166]\tvalidation_0-auc:0.68132\n[167]\tvalidation_0-auc:0.68173\n[168]\tvalidation_0-auc:0.68192\n[169]\tvalidation_0-auc:0.68155\n[170]\tvalidation_0-auc:0.68161\n[171]\tvalidation_0-auc:0.68173\n[172]\tvalidation_0-auc:0.68209\n[173]\tvalidation_0-auc:0.68301\n[174]\tvalidation_0-auc:0.68358\n[175]\tvalidation_0-auc:0.68348\n[176]\tvalidation_0-auc:0.68281\n[177]\tvalidation_0-auc:0.68256\n[178]\tvalidation_0-auc:0.68268\n[179]\tvalidation_0-auc:0.68299\n[180]\tvalidation_0-auc:0.68360\n[181]\tvalidation_0-auc:0.68284\n[182]\tvalidation_0-auc:0.68297\n[183]\tvalidation_0-auc:0.68269\n[184]\tvalidation_0-auc:0.68218\n[185]\tvalidation_0-auc:0.68247\n[186]\tvalidation_0-auc:0.68226\n[187]\tvalidation_0-auc:0.68212\n[188]\tvalidation_0-auc:0.68216\n[189]\tvalidation_0-auc:0.68196\n[190]\tvalidation_0-auc:0.68254\n[191]\tvalidation_0-auc:0.68277\n[192]\tvalidation_0-auc:0.68274\n[193]\tvalidation_0-auc:0.68275\n[194]\tvalidation_0-auc:0.68226\n[195]\tvalidation_0-auc:0.68239\n[196]\tvalidation_0-auc:0.68218\n[197]\tvalidation_0-auc:0.68125\n[198]\tvalidation_0-auc:0.68121\n[199]\tvalidation_0-auc:0.68144\n[200]\tvalidation_0-auc:0.68143\n[201]\tvalidation_0-auc:0.68172\n[202]\tvalidation_0-auc:0.68150\n[203]\tvalidation_0-auc:0.68099\n[204]\tvalidation_0-auc:0.68215\n[205]\tvalidation_0-auc:0.68158\n[206]\tvalidation_0-auc:0.68192\n[207]\tvalidation_0-auc:0.68240\n[208]\tvalidation_0-auc:0.68182\n[209]\tvalidation_0-auc:0.68207\n[210]\tvalidation_0-auc:0.68147\n[211]\tvalidation_0-auc:0.68166\n[212]\tvalidation_0-auc:0.68126\n[213]\tvalidation_0-auc:0.68109\n[214]\tvalidation_0-auc:0.68101\n[215]\tvalidation_0-auc:0.68060\n[216]\tvalidation_0-auc:0.68051\n[217]\tvalidation_0-auc:0.68099\n[218]\tvalidation_0-auc:0.68035\n[219]\tvalidation_0-auc:0.68004\n[220]\tvalidation_0-auc:0.68066\n[221]\tvalidation_0-auc:0.67971\n[222]\tvalidation_0-auc:0.67904\n[223]\tvalidation_0-auc:0.67929\n[224]\tvalidation_0-auc:0.67891\n[225]\tvalidation_0-auc:0.67887\n[226]\tvalidation_0-auc:0.67928\n[227]\tvalidation_0-auc:0.67967\n[228]\tvalidation_0-auc:0.67878\n[229]\tvalidation_0-auc:0.67883\n[230]\tvalidation_0-auc:0.67946\n[231]\tvalidation_0-auc:0.67919\n[232]\tvalidation_0-auc:0.67967\n[233]\tvalidation_0-auc:0.68005\n[234]\tvalidation_0-auc:0.68001\n[235]\tvalidation_0-auc:0.67993\n[236]\tvalidation_0-auc:0.67967\n[237]\tvalidation_0-auc:0.67957\n[238]\tvalidation_0-auc:0.67960\n[239]\tvalidation_0-auc:0.68005\n[240]\tvalidation_0-auc:0.67977\n[241]\tvalidation_0-auc:0.68001\n[242]\tvalidation_0-auc:0.68067\n[243]\tvalidation_0-auc:0.68065\n[244]\tvalidation_0-auc:0.68094\n[245]\tvalidation_0-auc:0.68058\n[246]\tvalidation_0-auc:0.68075\n[247]\tvalidation_0-auc:0.68105\n[248]\tvalidation_0-auc:0.68114\n[249]\tvalidation_0-auc:0.68112\n[250]\tvalidation_0-auc:0.68108\n[251]\tvalidation_0-auc:0.68129\n","output_type":"stream"},{"name":"stderr","text":"Default metric period is 5 because AUC is/are not implemented for GPU\n","output_type":"stream"},{"name":"stdout","text":"0:\ttest: 0.6063113\tbest: 0.6063113 (0)\ttotal: 141ms\tremaining: 1m 24s\n300:\ttest: 0.7177612\tbest: 0.7177612 (300)\ttotal: 28.1s\tremaining: 27.9s\n599:\ttest: 0.7263001\tbest: 0.7264063 (585)\ttotal: 56.3s\tremaining: 0us\nbestTest = 0.7264062762\nbestIteration = 585\nShrink model to first 586 iterations.\n[LightGBM] [Info] Number of positive: 1398, number of negative: 38807\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.903913\n[LightGBM] [Info] Total Bins 29212\n[LightGBM] [Info] Number of data points in the train set: 40205, number of used features: 373\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 185 dense feature groups (7.21 MB) transferred to GPU in 0.005900 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.034772 -> initscore=-3.323558\n[LightGBM] [Info] Start training from score -3.323558\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\nTraining until validation scores don't improve for 100 rounds\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 9\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 9\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 9\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 9\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 9\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 9\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 9\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Debug] Trained a tree with leaves = 57 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Debug] Trained a tree with leaves = 55 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Debug] Trained a tree with leaves = 61 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Debug] Trained a tree with leaves = 50 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Debug] Trained a tree with leaves = 62 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Debug] Trained a tree with leaves = 52 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Debug] Trained a tree with leaves = 61 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Debug] Trained a tree with leaves = 59 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Debug] Trained a tree with leaves = 62 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[200]\tvalid_0's auc: 0.74234\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Debug] Trained a tree with leaves = 58 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\nEarly stopping, best iteration is:\n[112]\tvalid_0's auc: 0.74576\n[0]\tvalidation_0-auc:0.48082\n[1]\tvalidation_0-auc:0.50826\n[2]\tvalidation_0-auc:0.55817\n[3]\tvalidation_0-auc:0.55956\n[4]\tvalidation_0-auc:0.56145\n[5]\tvalidation_0-auc:0.56157\n[6]\tvalidation_0-auc:0.56197\n[7]\tvalidation_0-auc:0.57034\n[8]\tvalidation_0-auc:0.59000\n[9]\tvalidation_0-auc:0.58813\n[10]\tvalidation_0-auc:0.59875\n[11]\tvalidation_0-auc:0.60521\n[12]\tvalidation_0-auc:0.61507\n[13]\tvalidation_0-auc:0.62408\n[14]\tvalidation_0-auc:0.62032\n[15]\tvalidation_0-auc:0.62419\n[16]\tvalidation_0-auc:0.62447\n[17]\tvalidation_0-auc:0.61962\n[18]\tvalidation_0-auc:0.61940\n[19]\tvalidation_0-auc:0.62271\n[20]\tvalidation_0-auc:0.62302\n[21]\tvalidation_0-auc:0.62730\n[22]\tvalidation_0-auc:0.63225\n[23]\tvalidation_0-auc:0.63534\n[24]\tvalidation_0-auc:0.63481\n[25]\tvalidation_0-auc:0.63783\n[26]\tvalidation_0-auc:0.64050\n[27]\tvalidation_0-auc:0.64163\n[28]\tvalidation_0-auc:0.64053\n[29]\tvalidation_0-auc:0.64123\n[30]\tvalidation_0-auc:0.64513\n[31]\tvalidation_0-auc:0.64549\n[32]\tvalidation_0-auc:0.64381\n[33]\tvalidation_0-auc:0.64688\n[34]\tvalidation_0-auc:0.64731\n[35]\tvalidation_0-auc:0.64789\n[36]\tvalidation_0-auc:0.64962\n[37]\tvalidation_0-auc:0.65194\n[38]\tvalidation_0-auc:0.65519\n[39]\tvalidation_0-auc:0.65425\n[40]\tvalidation_0-auc:0.65481\n[41]\tvalidation_0-auc:0.65574\n[42]\tvalidation_0-auc:0.65689\n[43]\tvalidation_0-auc:0.65725\n[44]\tvalidation_0-auc:0.65656\n[45]\tvalidation_0-auc:0.65640\n[46]\tvalidation_0-auc:0.65526\n[47]\tvalidation_0-auc:0.65433\n[48]\tvalidation_0-auc:0.65542\n[49]\tvalidation_0-auc:0.65568\n[50]\tvalidation_0-auc:0.65311\n[51]\tvalidation_0-auc:0.65395\n[52]\tvalidation_0-auc:0.65369\n[53]\tvalidation_0-auc:0.65273\n[54]\tvalidation_0-auc:0.65319\n[55]\tvalidation_0-auc:0.65474\n[56]\tvalidation_0-auc:0.65456\n[57]\tvalidation_0-auc:0.65517\n[58]\tvalidation_0-auc:0.65561\n[59]\tvalidation_0-auc:0.65696\n[60]\tvalidation_0-auc:0.65683\n[61]\tvalidation_0-auc:0.65787\n[62]\tvalidation_0-auc:0.65896\n[63]\tvalidation_0-auc:0.65805\n[64]\tvalidation_0-auc:0.65789\n[65]\tvalidation_0-auc:0.65960\n[66]\tvalidation_0-auc:0.65914\n[67]\tvalidation_0-auc:0.65941\n[68]\tvalidation_0-auc:0.65914\n[69]\tvalidation_0-auc:0.65848\n[70]\tvalidation_0-auc:0.65803\n[71]\tvalidation_0-auc:0.65598\n[72]\tvalidation_0-auc:0.65531\n[73]\tvalidation_0-auc:0.65546\n[74]\tvalidation_0-auc:0.65496\n[75]\tvalidation_0-auc:0.65442\n[76]\tvalidation_0-auc:0.65461\n[77]\tvalidation_0-auc:0.65464\n[78]\tvalidation_0-auc:0.65515\n[79]\tvalidation_0-auc:0.65481\n[80]\tvalidation_0-auc:0.65413\n[81]\tvalidation_0-auc:0.65517\n[82]\tvalidation_0-auc:0.65496\n[83]\tvalidation_0-auc:0.65563\n[84]\tvalidation_0-auc:0.65516\n[85]\tvalidation_0-auc:0.65668\n[86]\tvalidation_0-auc:0.65665\n[87]\tvalidation_0-auc:0.65793\n[88]\tvalidation_0-auc:0.65745\n[89]\tvalidation_0-auc:0.65835\n[90]\tvalidation_0-auc:0.65860\n[91]\tvalidation_0-auc:0.65873\n[92]\tvalidation_0-auc:0.65897\n[93]\tvalidation_0-auc:0.65866\n[94]\tvalidation_0-auc:0.65789\n[95]\tvalidation_0-auc:0.65760\n[96]\tvalidation_0-auc:0.65821\n[97]\tvalidation_0-auc:0.65795\n[98]\tvalidation_0-auc:0.65853\n[99]\tvalidation_0-auc:0.65920\n[100]\tvalidation_0-auc:0.65897\n[101]\tvalidation_0-auc:0.65853\n[102]\tvalidation_0-auc:0.65892\n[103]\tvalidation_0-auc:0.65817\n[104]\tvalidation_0-auc:0.65910\n[105]\tvalidation_0-auc:0.65882\n[106]\tvalidation_0-auc:0.65827\n[107]\tvalidation_0-auc:0.65902\n[108]\tvalidation_0-auc:0.65856\n[109]\tvalidation_0-auc:0.65810\n[110]\tvalidation_0-auc:0.65855\n[111]\tvalidation_0-auc:0.65928\n[112]\tvalidation_0-auc:0.65909\n[113]\tvalidation_0-auc:0.65855\n[114]\tvalidation_0-auc:0.65973\n[115]\tvalidation_0-auc:0.65971\n[116]\tvalidation_0-auc:0.65876\n[117]\tvalidation_0-auc:0.65947\n[118]\tvalidation_0-auc:0.66042\n[119]\tvalidation_0-auc:0.66053\n[120]\tvalidation_0-auc:0.66182\n[121]\tvalidation_0-auc:0.66133\n[122]\tvalidation_0-auc:0.66087\n[123]\tvalidation_0-auc:0.66025\n[124]\tvalidation_0-auc:0.66022\n[125]\tvalidation_0-auc:0.65954\n[126]\tvalidation_0-auc:0.66011\n[127]\tvalidation_0-auc:0.66047\n[128]\tvalidation_0-auc:0.65940\n[129]\tvalidation_0-auc:0.65932\n[130]\tvalidation_0-auc:0.66071\n[131]\tvalidation_0-auc:0.66050\n[132]\tvalidation_0-auc:0.66065\n[133]\tvalidation_0-auc:0.66154\n[134]\tvalidation_0-auc:0.66251\n[135]\tvalidation_0-auc:0.66282\n[136]\tvalidation_0-auc:0.66267\n[137]\tvalidation_0-auc:0.66283\n[138]\tvalidation_0-auc:0.66270\n[139]\tvalidation_0-auc:0.66161\n[140]\tvalidation_0-auc:0.66173\n[141]\tvalidation_0-auc:0.66230\n[142]\tvalidation_0-auc:0.66276\n[143]\tvalidation_0-auc:0.66320\n[144]\tvalidation_0-auc:0.66370\n[145]\tvalidation_0-auc:0.66275\n[146]\tvalidation_0-auc:0.66290\n[147]\tvalidation_0-auc:0.66270\n[148]\tvalidation_0-auc:0.66164\n[149]\tvalidation_0-auc:0.66098\n[150]\tvalidation_0-auc:0.66147\n[151]\tvalidation_0-auc:0.66133\n[152]\tvalidation_0-auc:0.66166\n[153]\tvalidation_0-auc:0.66229\n[154]\tvalidation_0-auc:0.66285\n[155]\tvalidation_0-auc:0.66256\n[156]\tvalidation_0-auc:0.66139\n[157]\tvalidation_0-auc:0.66100\n[158]\tvalidation_0-auc:0.66072\n[159]\tvalidation_0-auc:0.66068\n[160]\tvalidation_0-auc:0.66105\n[161]\tvalidation_0-auc:0.66095\n[162]\tvalidation_0-auc:0.66088\n[163]\tvalidation_0-auc:0.66113\n[164]\tvalidation_0-auc:0.66090\n[165]\tvalidation_0-auc:0.66167\n[166]\tvalidation_0-auc:0.66227\n[167]\tvalidation_0-auc:0.66210\n[168]\tvalidation_0-auc:0.66200\n[169]\tvalidation_0-auc:0.66221\n[170]\tvalidation_0-auc:0.66244\n[171]\tvalidation_0-auc:0.66242\n[172]\tvalidation_0-auc:0.66303\n[173]\tvalidation_0-auc:0.66331\n[174]\tvalidation_0-auc:0.66358\n[175]\tvalidation_0-auc:0.66431\n[176]\tvalidation_0-auc:0.66430\n[177]\tvalidation_0-auc:0.66436\n[178]\tvalidation_0-auc:0.66525\n[179]\tvalidation_0-auc:0.66447\n[180]\tvalidation_0-auc:0.66485\n[181]\tvalidation_0-auc:0.66592\n[182]\tvalidation_0-auc:0.66529\n[183]\tvalidation_0-auc:0.66482\n[184]\tvalidation_0-auc:0.66433\n[185]\tvalidation_0-auc:0.66427\n[186]\tvalidation_0-auc:0.66412\n[187]\tvalidation_0-auc:0.66493\n[188]\tvalidation_0-auc:0.66519\n[189]\tvalidation_0-auc:0.66488\n[190]\tvalidation_0-auc:0.66545\n[191]\tvalidation_0-auc:0.66631\n[192]\tvalidation_0-auc:0.66593\n[193]\tvalidation_0-auc:0.66563\n[194]\tvalidation_0-auc:0.66582\n[195]\tvalidation_0-auc:0.66558\n[196]\tvalidation_0-auc:0.66492\n[197]\tvalidation_0-auc:0.66449\n[198]\tvalidation_0-auc:0.66483\n[199]\tvalidation_0-auc:0.66451\n[200]\tvalidation_0-auc:0.66423\n[201]\tvalidation_0-auc:0.66408\n[202]\tvalidation_0-auc:0.66390\n[203]\tvalidation_0-auc:0.66458\n[204]\tvalidation_0-auc:0.66408\n[205]\tvalidation_0-auc:0.66376\n[206]\tvalidation_0-auc:0.66374\n[207]\tvalidation_0-auc:0.66373\n[208]\tvalidation_0-auc:0.66406\n[209]\tvalidation_0-auc:0.66393\n[210]\tvalidation_0-auc:0.66344\n[211]\tvalidation_0-auc:0.66320\n[212]\tvalidation_0-auc:0.66309\n[213]\tvalidation_0-auc:0.66292\n[214]\tvalidation_0-auc:0.66281\n[215]\tvalidation_0-auc:0.66292\n[216]\tvalidation_0-auc:0.66327\n[217]\tvalidation_0-auc:0.66296\n[218]\tvalidation_0-auc:0.66301\n[219]\tvalidation_0-auc:0.66290\n[220]\tvalidation_0-auc:0.66274\n[221]\tvalidation_0-auc:0.66306\n[222]\tvalidation_0-auc:0.66324\n[223]\tvalidation_0-auc:0.66261\n[224]\tvalidation_0-auc:0.66315\n[225]\tvalidation_0-auc:0.66333\n[226]\tvalidation_0-auc:0.66315\n[227]\tvalidation_0-auc:0.66325\n[228]\tvalidation_0-auc:0.66367\n[229]\tvalidation_0-auc:0.66343\n[230]\tvalidation_0-auc:0.66361\n[231]\tvalidation_0-auc:0.66364\n[232]\tvalidation_0-auc:0.66422\n[233]\tvalidation_0-auc:0.66499\n[234]\tvalidation_0-auc:0.66537\n[235]\tvalidation_0-auc:0.66524\n[236]\tvalidation_0-auc:0.66542\n[237]\tvalidation_0-auc:0.66529\n[238]\tvalidation_0-auc:0.66555\n[239]\tvalidation_0-auc:0.66567\n[240]\tvalidation_0-auc:0.66568\n[241]\tvalidation_0-auc:0.66596\n[242]\tvalidation_0-auc:0.66541\n[243]\tvalidation_0-auc:0.66528\n[244]\tvalidation_0-auc:0.66490\n[245]\tvalidation_0-auc:0.66457\n[246]\tvalidation_0-auc:0.66501\n[247]\tvalidation_0-auc:0.66528\n[248]\tvalidation_0-auc:0.66525\n[249]\tvalidation_0-auc:0.66619\n[250]\tvalidation_0-auc:0.66565\n[251]\tvalidation_0-auc:0.66630\n[252]\tvalidation_0-auc:0.66606\n[253]\tvalidation_0-auc:0.66628\n[254]\tvalidation_0-auc:0.66637\n[255]\tvalidation_0-auc:0.66682\n[256]\tvalidation_0-auc:0.66746\n[257]\tvalidation_0-auc:0.66724\n[258]\tvalidation_0-auc:0.66690\n[259]\tvalidation_0-auc:0.66708\n[260]\tvalidation_0-auc:0.66747\n[261]\tvalidation_0-auc:0.66778\n[262]\tvalidation_0-auc:0.66744\n[263]\tvalidation_0-auc:0.66745\n[264]\tvalidation_0-auc:0.66776\n[265]\tvalidation_0-auc:0.66789\n[266]\tvalidation_0-auc:0.66817\n[267]\tvalidation_0-auc:0.66778\n[268]\tvalidation_0-auc:0.66781\n[269]\tvalidation_0-auc:0.66773\n[270]\tvalidation_0-auc:0.66769\n[271]\tvalidation_0-auc:0.66780\n[272]\tvalidation_0-auc:0.66786\n[273]\tvalidation_0-auc:0.66819\n[274]\tvalidation_0-auc:0.66842\n[275]\tvalidation_0-auc:0.66807\n[276]\tvalidation_0-auc:0.66813\n[277]\tvalidation_0-auc:0.66859\n[278]\tvalidation_0-auc:0.66851\n[279]\tvalidation_0-auc:0.66837\n[280]\tvalidation_0-auc:0.66843\n[281]\tvalidation_0-auc:0.66857\n[282]\tvalidation_0-auc:0.66856\n[283]\tvalidation_0-auc:0.66864\n[284]\tvalidation_0-auc:0.66895\n[285]\tvalidation_0-auc:0.66932\n[286]\tvalidation_0-auc:0.66861\n[287]\tvalidation_0-auc:0.66859\n[288]\tvalidation_0-auc:0.66852\n[289]\tvalidation_0-auc:0.66906\n[290]\tvalidation_0-auc:0.66856\n[291]\tvalidation_0-auc:0.66816\n[292]\tvalidation_0-auc:0.66815\n[293]\tvalidation_0-auc:0.66854\n[294]\tvalidation_0-auc:0.66899\n[295]\tvalidation_0-auc:0.66884\n[296]\tvalidation_0-auc:0.66860\n[297]\tvalidation_0-auc:0.66882\n[298]\tvalidation_0-auc:0.66899\n[299]\tvalidation_0-auc:0.66848\n[300]\tvalidation_0-auc:0.66876\n[301]\tvalidation_0-auc:0.66861\n[302]\tvalidation_0-auc:0.66882\n[303]\tvalidation_0-auc:0.66822\n[304]\tvalidation_0-auc:0.66833\n[305]\tvalidation_0-auc:0.66853\n[306]\tvalidation_0-auc:0.66826\n[307]\tvalidation_0-auc:0.66860\n[308]\tvalidation_0-auc:0.66842\n[309]\tvalidation_0-auc:0.66803\n[310]\tvalidation_0-auc:0.66815\n[311]\tvalidation_0-auc:0.66819\n[312]\tvalidation_0-auc:0.66808\n[313]\tvalidation_0-auc:0.66807\n[314]\tvalidation_0-auc:0.66779\n[315]\tvalidation_0-auc:0.66837\n[316]\tvalidation_0-auc:0.66794\n[317]\tvalidation_0-auc:0.66837\n[318]\tvalidation_0-auc:0.66800\n[319]\tvalidation_0-auc:0.66791\n[320]\tvalidation_0-auc:0.66781\n[321]\tvalidation_0-auc:0.66826\n[322]\tvalidation_0-auc:0.66805\n[323]\tvalidation_0-auc:0.66723\n[324]\tvalidation_0-auc:0.66741\n[325]\tvalidation_0-auc:0.66764\n[326]\tvalidation_0-auc:0.66768\n[327]\tvalidation_0-auc:0.66776\n[328]\tvalidation_0-auc:0.66800\n[329]\tvalidation_0-auc:0.66808\n[330]\tvalidation_0-auc:0.66812\n[331]\tvalidation_0-auc:0.66804\n[332]\tvalidation_0-auc:0.66752\n[333]\tvalidation_0-auc:0.66761\n[334]\tvalidation_0-auc:0.66778\n[335]\tvalidation_0-auc:0.66821\n[336]\tvalidation_0-auc:0.66800\n[337]\tvalidation_0-auc:0.66763\n[338]\tvalidation_0-auc:0.66749\n[339]\tvalidation_0-auc:0.66786\n[340]\tvalidation_0-auc:0.66775\n[341]\tvalidation_0-auc:0.66745\n[342]\tvalidation_0-auc:0.66752\n[343]\tvalidation_0-auc:0.66716\n[344]\tvalidation_0-auc:0.66700\n[345]\tvalidation_0-auc:0.66703\n[346]\tvalidation_0-auc:0.66737\n[347]\tvalidation_0-auc:0.66756\n[348]\tvalidation_0-auc:0.66785\n[349]\tvalidation_0-auc:0.66736\n[350]\tvalidation_0-auc:0.66770\n[351]\tvalidation_0-auc:0.66760\n[352]\tvalidation_0-auc:0.66736\n[353]\tvalidation_0-auc:0.66733\n[354]\tvalidation_0-auc:0.66797\n[355]\tvalidation_0-auc:0.66785\n[356]\tvalidation_0-auc:0.66788\n[357]\tvalidation_0-auc:0.66768\n[358]\tvalidation_0-auc:0.66718\n[359]\tvalidation_0-auc:0.66719\n[360]\tvalidation_0-auc:0.66732\n[361]\tvalidation_0-auc:0.66765\n[362]\tvalidation_0-auc:0.66776\n[363]\tvalidation_0-auc:0.66761\n[364]\tvalidation_0-auc:0.66760\n[365]\tvalidation_0-auc:0.66764\n[366]\tvalidation_0-auc:0.66776\n[367]\tvalidation_0-auc:0.66750\n[368]\tvalidation_0-auc:0.66802\n[369]\tvalidation_0-auc:0.66783\n[370]\tvalidation_0-auc:0.66762\n[371]\tvalidation_0-auc:0.66776\n[372]\tvalidation_0-auc:0.66761\n[373]\tvalidation_0-auc:0.66783\n[374]\tvalidation_0-auc:0.66795\n[375]\tvalidation_0-auc:0.66809\n[376]\tvalidation_0-auc:0.66844\n[377]\tvalidation_0-auc:0.66823\n[378]\tvalidation_0-auc:0.66831\n[379]\tvalidation_0-auc:0.66789\n[380]\tvalidation_0-auc:0.66779\n[381]\tvalidation_0-auc:0.66786\n[382]\tvalidation_0-auc:0.66715\n[383]\tvalidation_0-auc:0.66757\n[384]\tvalidation_0-auc:0.66740\n[385]\tvalidation_0-auc:0.66717\n","output_type":"stream"},{"name":"stderr","text":"Default metric period is 5 because AUC is/are not implemented for GPU\n","output_type":"stream"},{"name":"stdout","text":"0:\ttest: 0.6155595\tbest: 0.6155595 (0)\ttotal: 143ms\tremaining: 1m 25s\n300:\ttest: 0.7249786\tbest: 0.7249786 (300)\ttotal: 28.4s\tremaining: 28.3s\n599:\ttest: 0.7302017\tbest: 0.7303635 (585)\ttotal: 56.4s\tremaining: 0us\nbestTest = 0.7303634882\nbestIteration = 585\nShrink model to first 586 iterations.\n[LightGBM] [Info] Number of positive: 1402, number of negative: 38397\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.900021\n[LightGBM] [Info] Total Bins 29275\n[LightGBM] [Info] Number of data points in the train set: 39799, number of used features: 369\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 185 dense feature groups (7.14 MB) transferred to GPU in 0.005928 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035227 -> initscore=-3.310080\n[LightGBM] [Info] Start training from score -3.310080\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\nTraining until validation scores don't improve for 100 rounds\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 9\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 9\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 9\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 9\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 9\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 9\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 9\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Debug] Trained a tree with leaves = 58 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Debug] Trained a tree with leaves = 63 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[200]\tvalid_0's auc: 0.74009\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Debug] Trained a tree with leaves = 60 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Debug] Trained a tree with leaves = 63 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Debug] Trained a tree with leaves = 44 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Debug] Trained a tree with leaves = 51 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\n[LightGBM] [Debug] Trained a tree with leaves = 64 and depth = 10\nEarly stopping, best iteration is:\n[150]\tvalid_0's auc: 0.741998\n[0]\tvalidation_0-auc:0.54820\n[1]\tvalidation_0-auc:0.54112\n[2]\tvalidation_0-auc:0.53056\n[3]\tvalidation_0-auc:0.55650\n[4]\tvalidation_0-auc:0.56125\n[5]\tvalidation_0-auc:0.58360\n[6]\tvalidation_0-auc:0.59329\n[7]\tvalidation_0-auc:0.59153\n[8]\tvalidation_0-auc:0.61378\n[9]\tvalidation_0-auc:0.62828\n[10]\tvalidation_0-auc:0.63172\n[11]\tvalidation_0-auc:0.64210\n[12]\tvalidation_0-auc:0.64652\n[13]\tvalidation_0-auc:0.64587\n[14]\tvalidation_0-auc:0.65018\n[15]\tvalidation_0-auc:0.64891\n[16]\tvalidation_0-auc:0.65222\n[17]\tvalidation_0-auc:0.64917\n[18]\tvalidation_0-auc:0.65313\n[19]\tvalidation_0-auc:0.65501\n[20]\tvalidation_0-auc:0.65383\n[21]\tvalidation_0-auc:0.65388\n[22]\tvalidation_0-auc:0.65258\n[23]\tvalidation_0-auc:0.65411\n[24]\tvalidation_0-auc:0.65452\n[25]\tvalidation_0-auc:0.64900\n[26]\tvalidation_0-auc:0.64948\n[27]\tvalidation_0-auc:0.64682\n[28]\tvalidation_0-auc:0.64654\n[29]\tvalidation_0-auc:0.64811\n[30]\tvalidation_0-auc:0.65042\n[31]\tvalidation_0-auc:0.65000\n[32]\tvalidation_0-auc:0.65105\n[33]\tvalidation_0-auc:0.65334\n[34]\tvalidation_0-auc:0.65393\n[35]\tvalidation_0-auc:0.65276\n[36]\tvalidation_0-auc:0.65227\n[37]\tvalidation_0-auc:0.64886\n[38]\tvalidation_0-auc:0.64725\n[39]\tvalidation_0-auc:0.64723\n[40]\tvalidation_0-auc:0.64807\n[41]\tvalidation_0-auc:0.64879\n[42]\tvalidation_0-auc:0.65062\n[43]\tvalidation_0-auc:0.64842\n[44]\tvalidation_0-auc:0.64918\n[45]\tvalidation_0-auc:0.64947\n[46]\tvalidation_0-auc:0.65039\n[47]\tvalidation_0-auc:0.65064\n[48]\tvalidation_0-auc:0.65276\n[49]\tvalidation_0-auc:0.65265\n[50]\tvalidation_0-auc:0.65246\n[51]\tvalidation_0-auc:0.65260\n[52]\tvalidation_0-auc:0.65281\n[53]\tvalidation_0-auc:0.65316\n[54]\tvalidation_0-auc:0.65136\n[55]\tvalidation_0-auc:0.65054\n[56]\tvalidation_0-auc:0.65202\n[57]\tvalidation_0-auc:0.65372\n[58]\tvalidation_0-auc:0.65442\n[59]\tvalidation_0-auc:0.65302\n[60]\tvalidation_0-auc:0.65311\n[61]\tvalidation_0-auc:0.65508\n[62]\tvalidation_0-auc:0.65785\n[63]\tvalidation_0-auc:0.65669\n[64]\tvalidation_0-auc:0.65748\n[65]\tvalidation_0-auc:0.65669\n[66]\tvalidation_0-auc:0.65691\n[67]\tvalidation_0-auc:0.65656\n[68]\tvalidation_0-auc:0.65552\n[69]\tvalidation_0-auc:0.65522\n[70]\tvalidation_0-auc:0.65630\n[71]\tvalidation_0-auc:0.65657\n[72]\tvalidation_0-auc:0.65714\n[73]\tvalidation_0-auc:0.65671\n[74]\tvalidation_0-auc:0.65776\n[75]\tvalidation_0-auc:0.65898\n[76]\tvalidation_0-auc:0.65854\n[77]\tvalidation_0-auc:0.65868\n[78]\tvalidation_0-auc:0.65925\n[79]\tvalidation_0-auc:0.65947\n[80]\tvalidation_0-auc:0.66066\n[81]\tvalidation_0-auc:0.65996\n[82]\tvalidation_0-auc:0.66124\n[83]\tvalidation_0-auc:0.66018\n[84]\tvalidation_0-auc:0.66017\n[85]\tvalidation_0-auc:0.65932\n[86]\tvalidation_0-auc:0.66115\n[87]\tvalidation_0-auc:0.66192\n[88]\tvalidation_0-auc:0.66120\n[89]\tvalidation_0-auc:0.66177\n[90]\tvalidation_0-auc:0.66146\n[91]\tvalidation_0-auc:0.66217\n[92]\tvalidation_0-auc:0.66338\n[93]\tvalidation_0-auc:0.66359\n[94]\tvalidation_0-auc:0.66391\n[95]\tvalidation_0-auc:0.66409\n[96]\tvalidation_0-auc:0.66439\n[97]\tvalidation_0-auc:0.66470\n[98]\tvalidation_0-auc:0.66537\n[99]\tvalidation_0-auc:0.66600\n[100]\tvalidation_0-auc:0.66584\n[101]\tvalidation_0-auc:0.66659\n[102]\tvalidation_0-auc:0.66705\n[103]\tvalidation_0-auc:0.66718\n[104]\tvalidation_0-auc:0.66696\n[105]\tvalidation_0-auc:0.66677\n[106]\tvalidation_0-auc:0.66815\n[107]\tvalidation_0-auc:0.66842\n[108]\tvalidation_0-auc:0.66881\n[109]\tvalidation_0-auc:0.66802\n[110]\tvalidation_0-auc:0.66894\n[111]\tvalidation_0-auc:0.66857\n[112]\tvalidation_0-auc:0.66909\n[113]\tvalidation_0-auc:0.66964\n[114]\tvalidation_0-auc:0.66868\n[115]\tvalidation_0-auc:0.66897\n[116]\tvalidation_0-auc:0.66928\n[117]\tvalidation_0-auc:0.66944\n[118]\tvalidation_0-auc:0.66929\n[119]\tvalidation_0-auc:0.66946\n[120]\tvalidation_0-auc:0.66897\n[121]\tvalidation_0-auc:0.66865\n[122]\tvalidation_0-auc:0.66817\n[123]\tvalidation_0-auc:0.66778\n[124]\tvalidation_0-auc:0.66811\n[125]\tvalidation_0-auc:0.66800\n[126]\tvalidation_0-auc:0.66840\n[127]\tvalidation_0-auc:0.66792\n[128]\tvalidation_0-auc:0.66862\n[129]\tvalidation_0-auc:0.66775\n[130]\tvalidation_0-auc:0.66700\n[131]\tvalidation_0-auc:0.66697\n[132]\tvalidation_0-auc:0.66691\n[133]\tvalidation_0-auc:0.66703\n[134]\tvalidation_0-auc:0.66686\n[135]\tvalidation_0-auc:0.66613\n[136]\tvalidation_0-auc:0.66567\n[137]\tvalidation_0-auc:0.66525\n[138]\tvalidation_0-auc:0.66555\n[139]\tvalidation_0-auc:0.66517\n[140]\tvalidation_0-auc:0.66585\n[141]\tvalidation_0-auc:0.66526\n[142]\tvalidation_0-auc:0.66657\n[143]\tvalidation_0-auc:0.66682\n[144]\tvalidation_0-auc:0.66648\n[145]\tvalidation_0-auc:0.66547\n[146]\tvalidation_0-auc:0.66540\n[147]\tvalidation_0-auc:0.66563\n[148]\tvalidation_0-auc:0.66644\n[149]\tvalidation_0-auc:0.66581\n[150]\tvalidation_0-auc:0.66546\n[151]\tvalidation_0-auc:0.66517\n[152]\tvalidation_0-auc:0.66525\n[153]\tvalidation_0-auc:0.66579\n[154]\tvalidation_0-auc:0.66581\n[155]\tvalidation_0-auc:0.66587\n[156]\tvalidation_0-auc:0.66587\n[157]\tvalidation_0-auc:0.66560\n[158]\tvalidation_0-auc:0.66547\n[159]\tvalidation_0-auc:0.66539\n[160]\tvalidation_0-auc:0.66451\n[161]\tvalidation_0-auc:0.66452\n[162]\tvalidation_0-auc:0.66504\n[163]\tvalidation_0-auc:0.66489\n[164]\tvalidation_0-auc:0.66485\n[165]\tvalidation_0-auc:0.66559\n[166]\tvalidation_0-auc:0.66576\n[167]\tvalidation_0-auc:0.66620\n[168]\tvalidation_0-auc:0.66630\n[169]\tvalidation_0-auc:0.66563\n[170]\tvalidation_0-auc:0.66534\n[171]\tvalidation_0-auc:0.66580\n[172]\tvalidation_0-auc:0.66543\n[173]\tvalidation_0-auc:0.66596\n[174]\tvalidation_0-auc:0.66658\n[175]\tvalidation_0-auc:0.66620\n[176]\tvalidation_0-auc:0.66681\n[177]\tvalidation_0-auc:0.66595\n[178]\tvalidation_0-auc:0.66616\n[179]\tvalidation_0-auc:0.66628\n[180]\tvalidation_0-auc:0.66645\n[181]\tvalidation_0-auc:0.66678\n[182]\tvalidation_0-auc:0.66695\n[183]\tvalidation_0-auc:0.66604\n[184]\tvalidation_0-auc:0.66589\n[185]\tvalidation_0-auc:0.66562\n[186]\tvalidation_0-auc:0.66526\n[187]\tvalidation_0-auc:0.66510\n[188]\tvalidation_0-auc:0.66536\n[189]\tvalidation_0-auc:0.66509\n[190]\tvalidation_0-auc:0.66491\n[191]\tvalidation_0-auc:0.66535\n[192]\tvalidation_0-auc:0.66529\n[193]\tvalidation_0-auc:0.66472\n[194]\tvalidation_0-auc:0.66455\n[195]\tvalidation_0-auc:0.66458\n[196]\tvalidation_0-auc:0.66449\n[197]\tvalidation_0-auc:0.66402\n[198]\tvalidation_0-auc:0.66423\n[199]\tvalidation_0-auc:0.66430\n[200]\tvalidation_0-auc:0.66409\n[201]\tvalidation_0-auc:0.66431\n[202]\tvalidation_0-auc:0.66486\n[203]\tvalidation_0-auc:0.66521\n[204]\tvalidation_0-auc:0.66567\n[205]\tvalidation_0-auc:0.66523\n[206]\tvalidation_0-auc:0.66530\n[207]\tvalidation_0-auc:0.66538\n[208]\tvalidation_0-auc:0.66511\n[209]\tvalidation_0-auc:0.66526\n[210]\tvalidation_0-auc:0.66494\n[211]\tvalidation_0-auc:0.66488\n[212]\tvalidation_0-auc:0.66529\nCV AUC scores:  [0.7385756166559331, 0.7393854384389006, 0.7717137748125441, 0.7264068706387545, 0.7303632080802793]\nMaximum CV AUC score:  0.7717137748125441\nCV AUC scores:  [0.7342033862525843, 0.7489462753237962, 0.7870657710859668, 0.7457595276435858, 0.7419982392795323]\nMaximum CV AUC score:  0.7870657710859668\nCV AUC scores:  [0.655016416501053, 0.6967317596547138, 0.68350929390374, 0.6693244383099455, 0.6696421454616834]\nMaximum CV AUC score:  0.6967317596547138\nCPU times: user 14min 7s, sys: 42 s, total: 14min 49s\nWall time: 7min 9s\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"15394"},"metadata":{}}]},{"cell_type":"code","source":"for idx_train, idx_valid in cv.split(df_train_d, y, groups=weeks):\n    #  DL part\n    x_train_d, y_train_d = df_train_d.iloc[idx_train], y.iloc[idx_train]\n    x_valid_d, y_valid_d = df_train_d.iloc[idx_valid], y.iloc[idx_valid]\n    \n    x_train_d  = torch.tensor(x_train_d.values, dtype=torch.float32).to(device)\n    x_valid_d  = torch.tensor(x_valid_d.values, dtype=torch.float32).to(device)\n    y_train_d  = torch.tensor(y_train_d.values, dtype=torch.float32).to(device).reshape(-1, 1)\n    y_valid_d  = torch.tensor(y_valid_d.values, dtype=torch.float32).reshape(-1, 1)\n    \n    print(x_train_d.shape)\n    print(y_train_d.shape)\n    print(x_valid_d.shape)\n    print(y_valid_d.shape)\n    \n    transformer_model = get_model('Transformer', x_train_d, device, batch_size)\n    PATH = f\"/kaggle/working/Transformer.pth\"\n#     prob, pred = train(transformer_model, x_train_d, y_train_d, x_valid_d, y_valid_d, num_epoch, lr, batch_size, weight_decay)\n    prob, pred, transformer_model = train(transformer_model, x_train_d, y_train_d, x_valid_d, y_valid_d, num_epoch, lr, batch_size, weight_decay)\n    \n    kan_model = get_model('KAN', x_train_d, device, batch_size)\n    PATH = f\"/kaggle/working/KAN.pth\"\n#     prob, pred = train(kan_model, x_train_d, y_train_d, x_valid_d, y_valid_d, num_epoch, lr, batch_size, weight_decay)\n    prob, pred, kan_model = train(kan_model, x_train_d, y_train_d, x_valid_d, y_valid_d, num_epoch, lr, batch_size, weight_decay)\n    \n    transformerKan_model = get_model('TransformerKAN', x_train_d, device, batch_size)\n    PATH = f\"/kaggle/working/TransformerKAN.pth\"\n#     prob, pred = train(transformerKan_model, x_train_d, y_train_d, x_valid_d, y_valid_d, num_epoch, lr, batch_size, weight_decay)\n    prob, pred, transformerKan_model = train(transformerKan_model, x_train_d, y_train_d, x_valid_d, y_valid_d, num_epoch, lr, batch_size, weight_decay)\n    \n    fitted_models_transformer.append(transformer_model)\n    fitted_models_kan.append(kan_model)\n    fitted_models_transformerkan.append(transformerKan_model)\n    \n# print(\"CV AUC scores: \", cv_scores_transformer)\n# print(\"Maximum CV AUC score: \", max(cv_scores_transformer))\n\n# print(\"CV AUC scores: \", cv_scores_kan)\n# print(\"Maximum CV AUC score: \", max(cv_scores_kan))\n\n# print(\"CV AUC scores: \", cv_scores_transformerkan)\n# print(\"Maximum CV AUC score: \", max(cv_scores_transformerkan))","metadata":{"execution":{"iopub.status.busy":"2024-05-27T21:08:39.561583Z","iopub.execute_input":"2024-05-27T21:08:39.562353Z","iopub.status.idle":"2024-05-27T21:14:04.490635Z","shell.execute_reply.started":"2024-05-27T21:08:39.562317Z","shell.execute_reply":"2024-05-27T21:14:04.489645Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"torch.Size([39413, 347])\ntorch.Size([39413, 1])\ntorch.Size([10587, 347])\ntorch.Size([10587, 1])\nTrain Transformer\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 154/154 [00:03<00:00, 49.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 42/42 [00:00<00:00, 91.99it/s] \n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6307832395341632\naccuracy: 0.967318409369982\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 154/154 [00:02<00:00, 55.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 42/42 [00:00<00:00, 92.00it/s] \n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6684806566374648\naccuracy: 0.967318409369982\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 154/154 [00:02<00:00, 57.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 42/42 [00:00<00:00, 92.15it/s] \n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.682556035385363\naccuracy: 0.9664683101917446\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 154/154 [00:02<00:00, 57.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 42/42 [00:00<00:00, 90.45it/s] \n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6666840699827792\naccuracy: 0.967318409369982\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 154/154 [00:02<00:00, 57.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 42/42 [00:00<00:00, 91.95it/s] \n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6003497784322679\naccuracy: 0.967318409369982\nTrain KAN\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 154/154 [00:01<00:00, 145.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 42/42 [00:00<00:00, 251.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6792203841184676\naccuracy: 0.9564560309813923\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 154/154 [00:01<00:00, 151.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 42/42 [00:00<00:00, 251.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6897487318626874\naccuracy: 0.9607065268725796\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 154/154 [00:01<00:00, 148.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 42/42 [00:00<00:00, 254.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6839438322553626\naccuracy: 0.9555114763389062\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 154/154 [00:01<00:00, 153.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 42/42 [00:00<00:00, 256.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6823490299956031\naccuracy: 0.963256824407292\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 154/154 [00:01<00:00, 153.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 42/42 [00:00<00:00, 253.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.7016763062223534\naccuracy: 0.9646736563710211\nSave new model\nTrain TransformerKAN\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 154/154 [00:07<00:00, 20.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 42/42 [00:00<00:00, 52.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.5995154070146465\naccuracy: 0.967318409369982\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 154/154 [00:07<00:00, 20.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 42/42 [00:00<00:00, 52.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.5844007680788941\naccuracy: 0.967318409369982\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 154/154 [00:07<00:00, 20.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 42/42 [00:00<00:00, 52.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6276682811299701\naccuracy: 0.967318409369982\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 154/154 [00:07<00:00, 20.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 42/42 [00:00<00:00, 52.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6595265093896066\naccuracy: 0.967318409369982\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 154/154 [00:07<00:00, 20.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 42/42 [00:00<00:00, 52.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6779154176259656\naccuracy: 0.9622178143005573\nSave new model\ntorch.Size([40440, 347])\ntorch.Size([40440, 1])\ntorch.Size([9560, 347])\ntorch.Size([9560, 1])\nTrain Transformer\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:02<00:00, 57.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 38/38 [00:00<00:00, 89.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6258022445532591\naccuracy: 0.9622384937238494\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:02<00:00, 57.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 38/38 [00:00<00:00, 89.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6461382198896122\naccuracy: 0.9622384937238494\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:02<00:00, 57.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 38/38 [00:00<00:00, 89.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6688824721704364\naccuracy: 0.9622384937238494\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:02<00:00, 57.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 38/38 [00:00<00:00, 90.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6867890915518639\naccuracy: 0.9622384937238494\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:02<00:00, 57.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 38/38 [00:00<00:00, 89.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6939143993430575\naccuracy: 0.9622384937238494\nSave new model\nTrain KAN\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:01<00:00, 151.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 38/38 [00:00<00:00, 252.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6968707305593557\naccuracy: 0.9510460251046026\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:01<00:00, 150.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 38/38 [00:00<00:00, 234.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.7049733516138542\naccuracy: 0.9583682008368201\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:01<00:00, 153.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 38/38 [00:00<00:00, 254.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6899202279905772\naccuracy: 0.9576359832635983\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:01<00:00, 129.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 38/38 [00:00<00:00, 251.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.7065630101308736\naccuracy: 0.9456066945606695\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:01<00:00, 154.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 38/38 [00:00<00:00, 251.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.7145692699947214\naccuracy: 0.9517782426778243\nSave new model\nTrain TransformerKAN\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:07<00:00, 20.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 38/38 [00:00<00:00, 52.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.5800778056388762\naccuracy: 0.9622384937238494\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:07<00:00, 20.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 38/38 [00:00<00:00, 51.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6175133753849554\naccuracy: 0.9622384937238494\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:07<00:00, 20.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 38/38 [00:00<00:00, 52.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6439765673674634\naccuracy: 0.9622384937238494\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:07<00:00, 20.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 38/38 [00:00<00:00, 52.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6708949154114368\naccuracy: 0.9622384937238494\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:07<00:00, 20.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 38/38 [00:00<00:00, 52.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6700397098444099\naccuracy: 0.9622384937238494\ntorch.Size([40143, 347])\ntorch.Size([40143, 1])\ntorch.Size([9857, 347])\ntorch.Size([9857, 1])\nTrain Transformer\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 57.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 39/39 [00:00<00:00, 90.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6337644442440907\naccuracy: 0.9644922390179568\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 57.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 39/39 [00:00<00:00, 90.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.5956248779095104\naccuracy: 0.9644922390179568\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 57.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 39/39 [00:00<00:00, 90.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.688725149889555\naccuracy: 0.9644922390179568\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 57.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 39/39 [00:00<00:00, 90.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.7097478549640115\naccuracy: 0.9644922390179568\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 56.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 39/39 [00:00<00:00, 90.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6736413770304588\naccuracy: 0.9643907882722939\nTrain KAN\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:01<00:00, 152.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 39/39 [00:00<00:00, 255.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.7308503508692843\naccuracy: 0.9607385614284265\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:01<00:00, 152.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 39/39 [00:00<00:00, 253.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.71951344122376\naccuracy: 0.9620574211220453\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:01<00:00, 153.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 39/39 [00:00<00:00, 253.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.748703211167711\naccuracy: 0.9552602211626255\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:01<00:00, 151.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 39/39 [00:00<00:00, 257.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.7554066928128147\naccuracy: 0.9609414629197525\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:01<00:00, 154.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 39/39 [00:00<00:00, 257.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.7405719094201265\naccuracy: 0.9468398092725981\nTrain TransformerKAN\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:07<00:00, 20.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 39/39 [00:00<00:00, 53.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.603815233887812\naccuracy: 0.9644922390179568\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:07<00:00, 20.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 39/39 [00:00<00:00, 53.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6331895295196022\naccuracy: 0.9644922390179568\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:07<00:00, 20.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 39/39 [00:00<00:00, 53.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6340162887496432\naccuracy: 0.9644922390179568\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:07<00:00, 20.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 39/39 [00:00<00:00, 52.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6674758448661887\naccuracy: 0.9644922390179568\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:07<00:00, 20.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 39/39 [00:00<00:00, 52.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.7245782806653744\naccuracy: 0.9644922390179568\nSave new model\ntorch.Size([40205, 347])\ntorch.Size([40205, 1])\ntorch.Size([9795, 347])\ntorch.Size([9795, 1])\nTrain Transformer\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:02<00:00, 53.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 39/39 [00:00<00:00, 121.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6285905988804539\naccuracy: 0.9647779479326187\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:02<00:00, 57.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 39/39 [00:00<00:00, 92.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.5062762058124377\naccuracy: 0.9647779479326187\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:02<00:00, 57.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 39/39 [00:00<00:00, 94.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.5777670424047235\naccuracy: 0.9647779479326187\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:02<00:00, 57.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 39/39 [00:00<00:00, 93.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6558914193696803\naccuracy: 0.9647779479326187\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:02<00:00, 57.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 39/39 [00:00<00:00, 93.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6189835135342381\naccuracy: 0.9647779479326187\nTrain KAN\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:01<00:00, 151.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 39/39 [00:00<00:00, 251.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6759515374587838\naccuracy: 0.9643695763144462\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:01<00:00, 153.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 39/39 [00:00<00:00, 255.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.692723564143854\naccuracy: 0.9632465543644717\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:01<00:00, 149.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 39/39 [00:00<00:00, 253.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6926315466605323\naccuracy: 0.962736089841756\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:01<00:00, 153.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 39/39 [00:00<00:00, 257.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6902648569894947\naccuracy: 0.946809596733027\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:01<00:00, 152.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 39/39 [00:00<00:00, 254.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6990660225442834\naccuracy: 0.9337417049515059\nSave new model\nTrain TransformerKAN\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:07<00:00, 20.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 39/39 [00:00<00:00, 54.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6155077064642281\naccuracy: 0.9647779479326187\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:07<00:00, 20.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 39/39 [00:00<00:00, 54.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.5892729085192854\naccuracy: 0.9647779479326187\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:07<00:00, 20.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 39/39 [00:00<00:00, 55.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6178644275745725\naccuracy: 0.9647779479326187\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:07<00:00, 20.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 39/39 [00:00<00:00, 55.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6564699026148302\naccuracy: 0.9647779479326187\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:07<00:00, 20.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 39/39 [00:00<00:00, 54.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6780225442834138\naccuracy: 0.9615109749872384\nSave new model\ntorch.Size([39799, 347])\ntorch.Size([39799, 1])\ntorch.Size([10201, 347])\ntorch.Size([10201, 1])\nTrain Transformer\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 156/156 [00:02<00:00, 57.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 40/40 [00:00<00:00, 91.74it/s] \n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6040753838192168\naccuracy: 0.966571904715224\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 156/156 [00:02<00:00, 57.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 40/40 [00:00<00:00, 91.67it/s] \n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6752586058187053\naccuracy: 0.966571904715224\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 156/156 [00:02<00:00, 57.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 40/40 [00:00<00:00, 91.83it/s] \n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6553446193929083\naccuracy: 0.9661797862954612\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 156/156 [00:02<00:00, 57.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 40/40 [00:00<00:00, 92.10it/s] \n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6797261365867007\naccuracy: 0.9663758455053426\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 156/156 [00:02<00:00, 57.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 40/40 [00:00<00:00, 91.85it/s] \n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6933018267474854\naccuracy: 0.966571904715224\nSave new model\nTrain KAN\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 156/156 [00:01<00:00, 152.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 40/40 [00:00<00:00, 254.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6836511751024608\naccuracy: 0.9659837270855799\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 156/156 [00:01<00:00, 152.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 40/40 [00:00<00:00, 242.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.7050516616799414\naccuracy: 0.9659837270855799\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 156/156 [00:01<00:00, 145.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 40/40 [00:00<00:00, 113.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.7081237025096215\naccuracy: 0.9604940692089011\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 156/156 [00:01<00:00, 155.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 40/40 [00:00<00:00, 251.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.7065256702337119\naccuracy: 0.9585334771100873\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 156/156 [00:01<00:00, 152.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 40/40 [00:00<00:00, 250.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.7106544407630582\naccuracy: 0.9600039211841976\nSave new model\nTrain TransformerKAN\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 156/156 [00:07<00:00, 20.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 40/40 [00:00<00:00, 53.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6323455057015221\naccuracy: 0.966571904715224\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 156/156 [00:07<00:00, 20.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 40/40 [00:00<00:00, 53.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6589903814696068\naccuracy: 0.966571904715224\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 156/156 [00:07<00:00, 20.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 40/40 [00:00<00:00, 53.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6813989697405911\naccuracy: 0.966571904715224\nSave new model\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 156/156 [00:07<00:00, 20.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 40/40 [00:00<00:00, 53.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6810416208145711\naccuracy: 0.966571904715224\ntraining\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 156/156 [00:07<00:00, 20.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 40/40 [00:00<00:00, 53.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"roc_auc_score: 0.6956172336464164\naccuracy: 0.966571904715224\nSave new model\n","output_type":"stream"}]},{"cell_type":"code","source":"class VotingModel(BaseEstimator, RegressorMixin):\n    def __init__(self, estimators):\n        super().__init__()\n        self.estimators = estimators\n        \n    def fit(self, X, y=None):\n        return self\n    \n#     def predict(self, X):\n#         y_preds = [estimator.predict(X) for estimator in self.estimators]\n#         return np.mean(y_preds, axis=0)\n    \n#     def predict_proba(self, X):\n#         y_preds = [estimator.predict_proba(X) for estimator in self.estimators]\n#         return np.mean(y_preds, axis=0)\n    \n    def predict_proba_with_dl(self, X1, X2):\n        batch_size = 256\n        while len(X2)%batch_size == 1:\n            batch_size += 32\n        y_preds = [estimator.predict_proba(X1)[:,1] for estimator in self.estimators[:5]]\n        \n        X1[cat_cols] = X1[cat_cols].astype(\"category\")\n        y_preds += [estimator.predict_proba(X1)[:,1] for estimator in self.estimators[5:15]]\n        y_preds += [predict(estimator, X2, batch_size) for estimator in self.estimators[15:]]\n        \n        y_preds = np.asarray(y_preds)\n#         print(y_preds.shape)\n#         print(y_preds)\n        return np.mean(y_preds, axis=0)\n    \n\nmodel = VotingModel(fitted_models_cat+fitted_models_lgb+fitted_models_xgb+\\\n                    fitted_models_transformer+fitted_models_kan+fitted_models_transformerkan)","metadata":{"papermill":{"duration":0.022829,"end_time":"2024-04-12T05:10:39.505874","exception":false,"start_time":"2024-04-12T05:10:39.483045","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-27T21:14:04.492225Z","iopub.execute_input":"2024-05-27T21:14:04.493067Z","iopub.status.idle":"2024-05-27T21:14:04.502331Z","shell.execute_reply.started":"2024-05-27T21:14:04.493030Z","shell.execute_reply":"2024-05-27T21:14:04.501318Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# Submision","metadata":{"papermill":{"duration":0.011022,"end_time":"2024-04-12T05:10:39.528","exception":false,"start_time":"2024-04-12T05:10:39.516978","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# df_test = df_test.drop(columns=[\"WEEK_NUM\"])\n# df_test = df_test.set_index(\"case_id\")\n# test_id = df_test[\"case_id\"]\n# test_week = df_test[\"WEEK_NUM\"]\n\ny_pred = pd.Series(model.predict_proba_with_dl(df_test, x_test_d), index=df_test.index)\n# print(y_pred)\ndf_subm = pd.read_csv(ROOT / \"sample_submission.csv\")\ndf_subm[\"score\"] = y_pred\ndf_subm[\"case_id\"] = test_id\ndf_subm = df_subm.set_index(\"case_id\")\n\ndf_subm.to_csv(\"submission.csv\")\ndf_subm","metadata":{"papermill":{"duration":0.728037,"end_time":"2024-04-12T05:10:40.267135","exception":false,"start_time":"2024-04-12T05:10:39.539098","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-27T21:14:04.503373Z","iopub.execute_input":"2024-05-27T21:14:04.503663Z","iopub.status.idle":"2024-05-27T21:14:05.576678Z","shell.execute_reply.started":"2024-05-27T21:14:04.503631Z","shell.execute_reply":"2024-05-27T21:14:05.575775Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"            score\ncase_id          \n57543    0.011780\n57549    0.018310\n57551    0.029097\n57552    0.020751\n57569    0.030649\n57630    0.070932\n57631    0.083569\n57632    0.033106\n57633    0.036539\n57634    0.071331","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>score</th>\n    </tr>\n    <tr>\n      <th>case_id</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>57543</th>\n      <td>0.011780</td>\n    </tr>\n    <tr>\n      <th>57549</th>\n      <td>0.018310</td>\n    </tr>\n    <tr>\n      <th>57551</th>\n      <td>0.029097</td>\n    </tr>\n    <tr>\n      <th>57552</th>\n      <td>0.020751</td>\n    </tr>\n    <tr>\n      <th>57569</th>\n      <td>0.030649</td>\n    </tr>\n    <tr>\n      <th>57630</th>\n      <td>0.070932</td>\n    </tr>\n    <tr>\n      <th>57631</th>\n      <td>0.083569</td>\n    </tr>\n    <tr>\n      <th>57632</th>\n      <td>0.033106</td>\n    </tr>\n    <tr>\n      <th>57633</th>\n      <td>0.036539</td>\n    </tr>\n    <tr>\n      <th>57634</th>\n      <td>0.071331</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}